# Venom v1.0 ğŸ

[![CI](https://github.com/mpieniak01/Venom/actions/workflows/ci.yml/badge.svg)](
https://github.com/mpieniak01/Venom/actions/workflows/ci.yml
)
[![GitGuardian](https://img.shields.io/badge/security-GitGuardian-blue)](https://www.gitguardian.com/)
[![Quality Gate Status](https://sonarcloud.io/api/project_badges/measure?project=mpieniak01_Venom&metric=alert_status)](
https://sonarcloud.io/summary/new_code?id=mpieniak01_Venom)

> **| [English Documentation Available](README.md)**

**Venom wersja desktop â€“ system meta-inteligencji** â€” Autonomiczny system agentÃ³w AI z warstwÄ… planowania strategicznego i ekspansjÄ… wiedzy.

Venom jest przeksztaÅ‚cany z prostego wykonawcy poleceÅ„ w **autonomicznego inÅ¼yniera**, ktÃ³ry potrafi:

### âœ¨ Kluczowe funkcjonalnoÅ›ci
- ğŸ¨ Tworzenie nowych narzÄ™dzi i autonaprawa
- ğŸ”Œ **Import narzÄ™dzi MCP** - Integracja z Model Context Protocol (import z Git)
- ğŸŒ **DostÄ™p do Internetu** - Wyszukiwanie aktualnych informacji (ceny, wiadomoÅ›ci, dokumentacja)
- ğŸ§  **Planowanie strategiczne** - Automatyczna dekompozycja zÅ‚oÅ¼onych projektÃ³w na kroki
- ğŸ“š **Synteza wiedzy** - Zbieranie i analiza dokumentacji z wielu ÅºrÃ³deÅ‚
- ğŸ¤– **ZarzÄ…dzanie agentami** - Koordynacja wielu wyspecjalizowanych agentÃ³w
- ğŸ’¾ **PamiÄ™Ä‡ dÅ‚ugoterminowa** - Zapisywanie i wykorzystywanie zdobytej wiedzy
- ğŸ“ **Uczenie przez obserwacjÄ™** - Nagrywanie demonstracji i automatyczne generowanie przepÅ‚ywÃ³w pracy (NOWOÅšÄ†!)
- ğŸ‘ğŸ‘ **PÄ™tla jakoÅ›ci** - informacje zwrotne uÅ¼ytkownika + logi i metryki jakoÅ›ci odpowiedzi
- ğŸ§  **Ukryte prompty** - zatwierdzone odpowiedzi jako skrÃ³ty kontekstu
- ğŸ§­ **Selekcja runtime LLM** - Ollama/vLLM + aktywny model sterowany z panelu
- ğŸ’¬ **CiÄ…gÅ‚oÅ›Ä‡ czatu** - spÃ³jna historia sesji per `session_id` (SessionStore), zachowywana po restartach backendu i podczas nawigacji
- ğŸ—ºï¸ **Wizualizacja pamiÄ™ci** - Warstwa pamiÄ™ci (LessonsStore + LanceDB) w widoku `/brain`, z filtrowaniem sesji/pinned i akcjami pin/usuÅ„
- ğŸ› ï¸ **Panel usÅ‚ug** - `/config` pokazuje realne statusy stosu lokalnego (Backend API, Next.js UI, Ollama, vLLM, LanceDB, Redis, Docker) + profile Full/Light/LLM OFF

## ğŸ–¼ï¸ PodglÄ…d interfejsu

### ğŸ§  Knowledge Grid â€” wizualizacja pamiÄ™ci i wiedzy
<p align="center">
  <img src="./docs/assets/wiedza.jpeg" width="900" />
</p>

### ğŸ§ª Trace Analysis â€” analiza przepÅ‚ywu Å¼Ä…daÅ„ i orkiestracji
<p align="center">
  <img src="./docs/assets/diagram.jpeg" width="900" />
</p>

### âš™ï¸ Konfiguracja â€” usÅ‚ugi runtime i profile uruchomieniowe
<p align="center">
  <img src="./docs/assets/konfiguracja.jpg" width="900" />
</p>

### ğŸ›ï¸ AI Command Center â€” konsola operacyjna i historia sesji
<p align="center">
  <img src="./docs/assets/chat.jpeg" width="900" />
</p>

### ğŸ¯ PrzykÅ‚ady uÅ¼ycia

```python
# 1. Wyszukiwanie aktualnych informacji
"Jaka jest aktualna cena Bitcoina?"
â†’ System automatycznie wyszukuje w Internecie i zwraca Å›wieÅ¼e dane

# 2. ZÅ‚oÅ¼one projekty z planowaniem
"StwÃ³rz grÄ™ Snake uÅ¼ywajÄ…c PyGame"
â†’ System:
  1. Znajdzie dokumentacjÄ™ PyGame (ResearcherAgent)
  2. Stworzy strukturÄ™ gry (CoderAgent)
  3. Dodaj logikÄ™ wÄ™Å¼a (CoderAgent)
  4. Zaimplementuje punktacjÄ™ (CoderAgent)

# 3. Strona webowa z wieloma plikami
"StwÃ³rz stronÄ™ HTML z zegarem cyfrowym i stylem CSS"
â†’ System utworzy osobno: index.html, style.css, script.js

# 4. NOWE: Uczenie przez demonstracjÄ™
"Venom, patrz jak wysyÅ‚am raport na Slacka"
â†’ [UÅ¼ytkownik wykonuje akcje]
â†’ System nagrywa, analizuje i generuje przepÅ‚yw pracy
â†’ "ZapisaÅ‚em jako umiejÄ™tnoÅ›Ä‡ 'wyslij_raport_slack'"
â†’ PÃ³Åºniej: "Venom, wyÅ›lij raport na Slacka" - wykonuje automatycznie!
```

## ğŸ—ï¸ Architektura

### Struktura projektu
```
venom_core/
â”œâ”€â”€ api/routes/          # Endpointy REST API (agenci, zadania, pamiÄ™Ä‡, wÄ™zÅ‚y)
â”œâ”€â”€ core/flows/          # PrzepÅ‚ywy biznesowe i orkiestracja
â”œâ”€â”€ agents/              # Wyspecjalizowani agenci AI
â”œâ”€â”€ execution/           # Warstwa wykonawcza i routing modeli
â”œâ”€â”€ perception/          # Percepcja (desktop_sensor, audio)
â”œâ”€â”€ memory/              # PamiÄ™Ä‡ dÅ‚ugoterminowa (wektory, graf, przepÅ‚ywy pracy)
â””â”€â”€ infrastructure/      # Infrastruktura (sprzÄ™t, chmura, broker wiadomoÅ›ci)
```

### GÅ‚Ã³wne komponenty

#### 1. **Warstwa strategiczna** (Planowanie)
- **ArchitectAgent** - Kierownik projektu, rozbija zÅ‚oÅ¼one zadania na kroki
- **ExecutionPlan** - Model planu wykonania ze zdefiniowanymi krokami i zaleÅ¼noÅ›ciami

#### 2. **Ekspansja wiedzy**
- **ResearcherAgent** - Zbiera i syntetyzuje wiedzÄ™ z Internetu
- **WebSearchSkill** - Wyszukiwanie (DuckDuckGo) i scraping (trafilatura)
- **MemorySkill** - PamiÄ™Ä‡ dÅ‚ugoterminowa (LanceDB)

#### 3. **Warstwa wykonawcza**
- **CoderAgent** - Generuje kod z wykorzystaniem wiedzy
- **CriticAgent** - Weryfikuje jakoÅ›Ä‡ kodu
- **LibrarianAgent** - ZarzÄ…dza plikami i strukturÄ… projektu
- **ChatAgent** - Rozmowa i asystent
- **GhostAgent** - Automatyzacja GUI (RPA - Robotic Process Automation)
- **ApprenticeAgent** - Uczenie siÄ™ przepÅ‚ywÃ³w pracy poprzez obserwacjÄ™ (NOWOÅšÄ†!)

#### 4. **Silnik hybrydowy AI** ğŸ§ 
- **HybridModelRouter** (`venom_core/execution/model_router.py`) - Inteligentny routing miÄ™dzy lokalnym LLM a chmurÄ…
- **Tryby pracy**: LOCAL (tylko lokalne), HYBRID (mix), CLOUD (gÅ‚Ã³wnie chmura)
- **Lokalnie najpierw**: PrywatnoÅ›Ä‡ i $0 kosztÃ³w operacyjnych
- **Providerzy**: Ollama/vLLM (lokalne), Google Gemini, OpenAI
- WraÅ¼liwe dane **NIGDY** nie trafiajÄ… do chmury
- **Runtime jako API**: silnik modeli jest traktowany jak wymienialny serwer HTTP â€” moÅ¼emy go uruchamiaÄ‡ lub nie, bez wpÅ‚ywu na logikÄ™ bazowÄ…. To pozwala korzystaÄ‡ z rÃ³Å¼nych standardÃ³w modeli.
- **Kierunek LLM-first (Ollama)**: w trybie single-user i niskiego natÄ™Å¼enia zapytaÅ„ wydajnoÅ›Ä‡ Ollamy jest w praktyce porÃ³wnywalna do vLLM, a przeÅ‚Ä…czanie modeli jest prostsze. vLLM zyskuje przewagÄ™ gÅ‚Ã³wnie przy duÅ¼ej rÃ³wnolegÅ‚oÅ›ci i wysokim obciÄ…Å¼eniu.

#### 5. **Uczenie przez demonstracjÄ™** ğŸ“
- **DemonstrationRecorder** - Nagrywanie akcji uÅ¼ytkownika (mysz, klawiatura, zrzuty ekranu)
- **DemonstrationAnalyzer** - Analiza behawioralna i transformacja pikseli â†’ semantyka
- **WorkflowStore** - Magazyn procedur z moÅ¼liwoÅ›ciÄ… edycji
- **Integracja z GhostAgent** - Wykonywanie wygenerowanych przepÅ‚ywÃ³w pracy

#### 6. **Orkiestracja**
- **Orchestrator** - GÅ‚Ã³wny koordynator systemu
- **IntentManager** - Klasyfikacja intencji (5 typÃ³w: CODE_GENERATION, RESEARCH, COMPLEX_PLANNING, KNOWLEDGE_SEARCH, GENERAL_CHAT)
- **TaskDispatcher** - Routing zadaÅ„ do odpowiednich agentÃ³w

#### 7. **UsÅ‚ugi runtime (operacyjne)**
- **Backend API** (FastAPI/uvicorn) i **Next.js UI** â€“ podstawowe procesy.
- **Serwery LLM**: Ollama, vLLM â€“ start/stop z panelu usÅ‚ug.
- **LanceDB** â€“ lokalna pamiÄ™Ä‡ wektorowa (embedded); **Redis** â€“ opcjonalny broker/locki (moÅ¼e byÄ‡ wyÅ‚Ä…czony).
- **Nexus**, **Background Tasks** â€“ opcjonalne miejsca na przyszÅ‚e procesy (domyÅ›lnie disabled, bez akcji start/stop; moÅ¼na ukryÄ‡/ignorowaÄ‡ jeÅ›li niewykorzystane).

**Uwaga o vision/obrazie:** percepcja korzysta obecnie z lokalnych modeli ONNX (OCR/rozpoznawanie obiektÃ³w) oraz wybranych Å›cieÅ¼ek audio. Multimodalne LLM-y (Ollama/vLLM) sÄ… wspierane koncepcyjnie, ale nie sÄ… jeszcze spiÄ™te jako runtime vision.

### PrzepÅ‚yw danych

```
Zapytanie uÅ¼ytkownika
    â†“
IntentManager (klasyfikacja intencji)
    â†“
Orchestrator (decyzja o przepÅ‚ywie)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prosty kod         â”‚  ZÅ‚oÅ¼ony projekt    â”‚  Wyszukiwanie        â”‚
â”‚  CODE_GENERATION    â”‚  COMPLEX_PLANNING   â”‚  RESEARCH            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    CoderAgent       â”‚  ArchitectAgent     â”‚   ResearcherAgent    â”‚
â”‚         â†“           â”‚         â†“           â”‚        â†“             â”‚
â”‚    CriticAgent      â”‚  Tworzenie planu    â”‚   WebSearchSkill     â”‚
â”‚         â†“           â”‚         â†“           â”‚        â†“             â”‚
â”‚       Wynik         â”‚  Wykonanie planu    â”‚   MemorySkill        â”‚
â”‚                     â”‚   (krok po kroku)   â”‚        â†“             â”‚
â”‚                     â”‚        â†“            â”‚      Wynik           â”‚
â”‚                     â”‚      Wynik          â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Szybki start

> ğŸ” **Nowy dashboard web-next**
> SzczegÃ³Å‚owy opis ÅºrÃ³deÅ‚ danych dla widokÃ³w Brain/Strategy oraz checklistÄ™ testÃ³w znajdziesz w `docs/PL/FRONTEND_NEXT_GUIDE.md`. Dokument definiuje teÅ¼ kryteria wejÅ›cia do kolejnego etapu prac nad UI.
> Dokumentacja sesji chatu, trybÃ³w Direct/Normal/Complex i zachowania pamiÄ™ci: `docs/PL/CHAT_SESSION.md`.
> Dokumentacja standardÃ³w Skills oraz importu MCP: `docs/PL/DEV_GUIDE_SKILLS.md`.

## ğŸ–¥ï¸ Frontend (Next.js â€“ `web-next`)

Nowa warstwa prezentacji dziaÅ‚a na Next.js 15 (App Router, React 19). Interfejs jest zÅ‚oÅ¼ony z dwÃ³ch typÃ³w komponentÃ³w:
- **SCC (komponenty serwerowe/klienckie)** â€“ domyÅ›lnie tworzymy komponenty serwerowe (bez dyrektywy `"use client"`), a interaktywne fragmenty oznaczamy jako klientowe. DziÄ™ki temu widoki Brain/Strategy i Cockpit mogÄ… strumieniowaÄ‡ dane bez dodatkowych zapytaÅ„.
- **WspÃ³lny layout** (`components/layout/*`) â€“ TopBar, Sidebar, dolna belka statusu oraz overlaye dzielÄ… tokeny graficzne i tÅ‚umaczenia (`useTranslation`).

### Kluczowe komendy

```bash
# instalacja zaleÅ¼noÅ›ci
npm --prefix web-next install

# Å›rodowisko developerskie (http://localhost:3000)
npm --prefix web-next run dev

# build produkcyjny (generuje meta wersje + standalone)
npm --prefix web-next run build

# krÃ³tkie testy E2E (Playwright, tryb prod)
npm --prefix web-next run test:e2e

# walidacja spÃ³jnoÅ›ci tÅ‚umaczeÅ„
npm --prefix web-next run lint:locales
```

Skrypt `predev/prebuild` uruchamia `scripts/generate-meta.mjs`, ktÃ³ry zapisuje `public/meta.json` (wersja + skrÃ³t commitu). Wszystkie hooki HTTP korzystajÄ… z `lib/api-client.ts`; w trybie lokalnym moÅ¼esz wskazaÄ‡ backend przez zmienne:

```
NEXT_PUBLIC_API_BASE=http://localhost:8000
NEXT_PUBLIC_WS_BASE=ws://localhost:8000/ws/events
API_PROXY_TARGET=http://localhost:8000
```

> SzczegÃ³Å‚y (architektura katalogÃ³w, wytyczne dla SCC, ÅºrÃ³dÅ‚a danych widokÃ³w) opisuje `docs/PL/FRONTEND_NEXT_GUIDE.md`.

Uwaga: Cockpit ma teraz dwa widoki â€” `/` (produkcyjny ukÅ‚ad z wybranymi boxami) oraz `/chat` (referencyjna, peÅ‚na kopia wczeÅ›niejszego ukÅ‚adu).

#### Slash commands w Cockpit
- Wymuszenie narzÄ™dzia: `/<tool>` (np. `/git`, `/web`).
- Wymuszenie providerÃ³w: `/gpt` (OpenAI) i `/gem` (Gemini).
- Po wykryciu prefiksu treÅ›Ä‡ zapytania jest czyszczona z dyrektywy, a UI pokazuje etykietÄ™ "Forced".
- Ustawienie jÄ™zyka UI (PL/EN/DE) jest przekazywane jako `preferred_language` w `/api/v1/tasks`.
- Strategia streszczeÅ„ kontekstu (`SUMMARY_STRATEGY` w `.env`): `llm_with_fallback` (domyÅ›lnie, aktywny model) lub `heuristic_only`.

### Instalacja

```bash
# Klonowanie repozytorium
git clone https://github.com/mpieniak01/Venom.git
cd Venom

# Instalacja zaleÅ¼noÅ›ci
pip install -r requirements.txt

# Konfiguracja (skopiuj .env.example do .env i uzupeÅ‚nij)
cp .env.example .env
```

### Wymagane zaleÅ¼noÅ›ci

```
Python 3.10+ (zalecane 3.11)
```

### Kluczowe pakiety:
- `semantic-kernel>=1.9.0` - Orkiestracja agentÃ³w
- `ddgs>=1.0` - Wyszukiwarka (nastÄ™pca duckduckgo-search)
- `trafilatura` - Ekstrakcja tekstu ze stron WWW
- `beautifulsoup4` - Parsowanie HTML
- `lancedb` - Baza wektorowa dla pamiÄ™ci
- `fastapi` - API serwera
- `zeroconf` - wykrywanie usÅ‚ug mDNS dla lokalnej sieci
- `pynput` - Nagrywanie akcji uÅ¼ytkownika (THE_APPRENTICE)
- `google-generativeai` - Google Gemini (opcjonalne)
- `openai` / `anthropic` - Modele LLM (opcjonalne)

PeÅ‚na lista w [requirements.txt](requirements.txt)

### Konfiguracja

StwÃ³rz plik `.env` na podstawie `.env.example`:

```bash
cp .env.example .env
```

## âš™ï¸ Uruchamianie (FastAPI + Next.js)

PeÅ‚na lista krokÃ³w oraz checklisty wdroÅ¼eniowej znajduje siÄ™ w [`docs/PL/DEPLOYMENT_NEXT.md`](docs/PL/DEPLOYMENT_NEXT.md). PoniÅ¼ej skrÃ³t:

### Tryb developerski
```bash
# backend (uvicorn --reload) + web-next (next dev, turbopack off)
make start        # alias make start-dev

# zatrzymanie procesÃ³w i czyszczenie portÃ³w 8000/3000
make stop

# status PID-Ã³w
make status
```

### Tryb produkcyjny
```bash
make start-prod   # build next + uvicorn bez reload
make stop
```

- backend dziaÅ‚a na `http://localhost:8000` (REST/SSE/WS),
- Next.js serwuje UI na `http://localhost:3000`.

### ğŸ”§ Profile uruchomieniowe (tryb lekki)

Venom oferuje elastyczne tryby uruchamiania komponentÃ³w osobno - idealnie dla Å›rodowisk developerskich z ograniczonymi zasobami (PC, laptop).

#### Uruchamianie komponentÃ³w osobno

| Komenda | Opis | ZuÅ¼ycie zasobÃ³w | Kiedy uÅ¼ywaÄ‡ |
|---------|------|-----------------|--------------|
| `make api` | Backend (produkcyjny, **bez** automatycznego przeÅ‚adowania) | ~50 MB RAM, ~5% CPU | Praca nad frontendem lub gdy nie edytujesz kodu backendu |
| `make api-dev` | Backend (developerski, **z** automatycznym przeÅ‚adowaniem) | ~110 MB RAM, ~70% CPU (skoki) | Aktywna praca nad kodem backendu |
| `make api-stop` | Zatrzymaj tylko backend | - | Zwalnia port 8000 i pamiÄ™Ä‡ backendu |
| `make web` | Frontend (produkcyjny build + start) | ~500 MB RAM, ~3% CPU | Demo lub gdy nie edytujesz UI |
| `make web-dev` | Frontend (dev server z automatycznym przeÅ‚adowaniem) | ~1.3 GB RAM, ~7% CPU | Aktywna praca nad UI |
| `make web-stop` | Zatrzymaj tylko frontend | - | Zwalnia port 3000 i pamiÄ™Ä‡ frontend |
| `make vllm-start` | Uruchom vLLM (lokalny model LLM) | ~1.4 GB RAM, 13% RAM | Tylko gdy pracujesz z lokalnymi modelami |
| `make vllm-stop` | Zatrzymaj vLLM | - | Zwalnia ~1.4 GB RAM |
| `make ollama-start` | Uruchom Ollama | ~400 MB RAM | Alternatywa dla vLLM |
| `make ollama-stop` | Zatrzymaj Ollama | - | Zwalnia pamiÄ™Ä‡ Ollama |

#### PrzykÅ‚adowe scenariusze uÅ¼ycia

**Scenariusz 1: Praca tylko nad API (Light)**
```bash
make api          # Backend bez automatycznego przeÅ‚adowania (~50 MB)
# Nie uruchamiaj web ani LLM - oszczÄ™dzasz ~2.7 GB RAM
```

**Scenariusz 2: Praca nad frontendem**
```bash
make api          # Backend w tle (stabilny, bez reload)
make web-dev      # Frontend z automatycznym przeÅ‚adowaniem do pracy nad UI
# Nie uruchamiaj LLM jeÅ›li nie jest potrzebny
```

**Scenariusz 3: PeÅ‚ny stack development**
```bash
make api-dev      # Backend z automatycznym przeÅ‚adowaniem
make web-dev      # Frontend z automatycznym przeÅ‚adowaniem
make vllm-start   # LLM tylko jeÅ›li pracujesz z lokalnymi modelami
```

**Scenariusz 4: Demo / prezentacja**
```bash
make start-prod   # Wszystko w trybie produkcyjnym (niÅ¼sze zuÅ¼ycie CPU)
```

**Scenariusz 5: Tylko testowanie API**
```bash
make api          # Backend bez UI
curl http://localhost:8000/health
```

#### ğŸ’¡ WskazÃ³wki optymalizacji

- **VS Code Server**: JeÅ›li pracujesz w CLI, zamknij zdalne VS Code:
  ```bash
  # Z poziomu WSL/Linux
  pkill -f vscode-server
  # Lub jeÅ›li uÅ¼ywasz code tunnel
  code tunnel exit
  ```

- **Autoreload**: `--reload` w uvicorn spawnuje dodatkowy proces watchera. UÅ¼ywaj `make api` zamiast `make api-dev` gdy nie edytujesz kodu backendu.

- **Next.js dev**: `next dev` zuÅ¼ywa ~1.3 GB RAM przez automatyczne przeÅ‚adowanie. UÅ¼ywaj `make web` (produkcyjny) gdy tylko testujesz, nie edytujesz UI.

- **Åšrodowisko LLM**: vLLM/Ollama zuÅ¼ywajÄ… 1-2 GB RAM. Uruchamiaj je **tylko** gdy pracujesz z lokalnymi modelami. W trybie `AI_MODE=CLOUD` nie sÄ… potrzebne.

> Wszystkie dane i testy sÄ… traktowane jako lokalny eksperyment â€“ Venom dziaÅ‚a na prywatnej maszynie uÅ¼ytkownika i **nie szyfrujemy artefaktÃ³w**. Zamiast tego katalogi z wynikami (`**/test-results/`, `perf-artifacts/`, raporty Playwright/Locust) trafiajÄ… na listÄ™ `.gitignore`, aby uniknÄ…Ä‡ przypadkowego commitowania wraÅ¼liwych danych. Transparencja ma priorytet nad formalnymi â€danymi typu shadowâ€.

#### Kluczowe zmienne Å›rodowiskowe:

**Konfiguracja AI (silnik hybrydowy):**
```bash
# Tryb AI: LOCAL (tylko lokalne), HYBRID (mix), CLOUD (gÅ‚Ã³wnie chmura)
AI_MODE=LOCAL

# Lokalne LLM (Ollama/vLLM)
LLM_SERVICE_TYPE=local
LLM_LOCAL_ENDPOINT=http://localhost:11434/v1
LLM_MODEL_NAME=llama3

# Dostawcy chmurowi (opcjonalne, wymagane dla HYBRID/CLOUD)
GOOGLE_API_KEY=your_key_here
OPENAI_API_KEY=your_key_here

# Ustawienia hybrydowe
HYBRID_CLOUD_PROVIDER=google        # google lub openai
HYBRID_LOCAL_MODEL=llama3
HYBRID_CLOUD_MODEL=gemini-1.5-pro
SENSITIVE_DATA_LOCAL_ONLY=true     # WraÅ¼liwe dane ZAWSZE lokalnie

# vLLM (lokalny runtime OpenAI-compatible)
VLLM_MODEL_PATH=models/gemma-3-4b-it
VLLM_SERVED_MODEL_NAME=gemma-3-4b-it
VLLM_HOST=0.0.0.0
VLLM_PORT=8001
VLLM_GPU_MEMORY_UTILIZATION=0.85
VLLM_MAX_BATCHED_TOKENS=128
VLLM_MAX_MODEL_LEN=1024
VLLM_MAX_NUM_SEQS=2
VLLM_START_COMMAND="bash ./scripts/llm/vllm_service.sh start"
VLLM_STOP_COMMAND="bash ./scripts/llm/vllm_service.sh stop"
VLLM_RESTART_COMMAND="bash ./scripts/llm/vllm_service.sh restart"

# SpÃ³jny profil generacji dla Gemma 3 (vLLM + Ollama)
MODEL_GENERATION_OVERRIDES={"vllm":{"gemma-3-4b-it":{"temperature":0.3,"top_p":0.9,"max_tokens":800}},"ollama":{"gemma3:4b":{"temperature":0.3,"top_p":0.9,"num_predict":800,"num_ctx":1024}}}
```

### Test szybkoÅ›ci LLM (vLLM vs Ollama)
- Skrypt: `scripts/bench/compare_llm.py` (porÃ³wnuje TTFT/czas/tokeny na 3 promptach). Startuje vLLM, wykonuje test, zatrzymuje vLLM, nastÄ™pnie (jeÅ›li Ollama nie dziaÅ‚a) uruchamia OllamÄ™, testuje i jÄ… wyÅ‚Ä…cza â€“ tak by Å›rodowisko wrÃ³ciÅ‚o do stanu wyjÅ›ciowego. DomyÅ›lnie `BENCH_FORCE_CLEANUP=1`, wiÄ™c po teÅ›cie oba serwery sÄ… zatrzymywane.
- Uwaga: uruchamiaj na czystym Å›rodowisku (bez rÃ³wnolegÅ‚ego Venoma); do dodatkowej kontroli moÅ¼esz ustawiÄ‡ `OLLAMA_START_COMMAND`, `OLLAMA_STOP_COMMAND`, `VLLM_START_COMMAND`, `VLLM_STOP_COMMAND`, `BENCH_FORCE_CLEANUP`.
- WywoÅ‚anie:
  ```bash
  cd /home/ubuntu/venom
  source .venv/bin/activate
  python3 scripts/bench/compare_llm.py
  ```
  Wyniki sÄ… drukowane w dwÃ³ch tabelach (vLLM i Ollama) oraz w formacie JSON.

**SieÄ‡ i wykrywanie (lokalnie najpierw):**
```bash
# mDNS (Zeroconf) dla lokalnej sieci - venom.local
# UWAGA: Cloudflare zostaÅ‚ usuniÄ™ty, uÅ¼ywamy lokalnego wykrywania
```

**The Hive (przetwarzanie rozproszone):**
```bash
ENABLE_HIVE=false
HIVE_URL=https://hive.example.com:8080
HIVE_REGISTRATION_TOKEN=your_token
REDIS_HOST=localhost
```

**The Nexus (siatka rozproszona):**
```bash
ENABLE_NEXUS=false
NEXUS_SHARED_TOKEN=your_secret_token
NEXUS_PORT=8765
```

**Integracje zewnÄ™trzne:**
```bash
GITHUB_TOKEN=ghp_your_token         # Token dostÄ™pu osobistego
GITHUB_REPO_NAME=username/repo      # Nazwa repozytorium
DISCORD_WEBHOOK_URL=https://...     # Opcjonalne
SLACK_WEBHOOK_URL=https://...       # Opcjonalne
HF_TOKEN=                           # Opcjonalne (Hugging Face)
TAVILY_API_KEY=                     # Opcjonalne (Tavily Search)
ENABLE_GOOGLE_CALENDAR=false        # Opcjonalne
GOOGLE_CALENDAR_CREDENTIALS_PATH=./config/google_calendar_credentials.json
GOOGLE_CALENDAR_TOKEN_PATH=./config/google_calendar_token.json
ENABLE_ISSUE_POLLING=false          # WÅ‚Ä…cz automatyczne odpytywanie zgÅ‚oszeÅ„ Issue
```

ğŸ“– **PeÅ‚na lista zmiennych:** [.env.example](.env.example)
ğŸ“– **Dokumentacja integracji zewnÄ™trznych:** [docs/PL/EXTERNAL_INTEGRATIONS.md](docs/PL/EXTERNAL_INTEGRATIONS.md)
ğŸ“– **Dokumentacja silnika hybrydowego AI:** [docs/PL/HYBRID_AI_ENGINE.md](docs/PL/HYBRID_AI_ENGINE.md)

### Panel konfiguracji (UI)

Venom 2.0 wprowadza **graficzny panel konfiguracji** dostÄ™pny w interfejsie webowym pod adresem `http://localhost:3000/config`. Panel umoÅ¼liwia:

#### ZarzÄ…dzanie usÅ‚ugami
- **Monitorowanie statusÃ³w** - Backend, UI, LLM (Ollama/vLLM), Hive, Nexus, zadania w tle
- **Kontrola procesÃ³w** - Uruchamianie/zatrzymywanie/restart z UI bez koniecznoÅ›ci korzystania z terminala
- **Metryki w czasie rzeczywistym** - PID, port, CPU%, RAM, czas dziaÅ‚ania, ostatnie logi
- **Profile szybkie**:
  - `Full Stack` - Wszystkie usÅ‚ugi aktywne
  - `Light` - Tylko Backend i UI (oszczÄ™dnoÅ›Ä‡ zasobÃ³w)
  - `LLM OFF` - Wszystko oprÃ³cz modeli jÄ™zykowych

#### Edycja parametrÃ³w
Panel umoÅ¼liwia edycjÄ™ kluczowych parametrÃ³w runtime z poziomu UI, z automatycznÄ…:
- **WalidacjÄ… zakresÃ³w** - Porty (1-65535), progi pewnoÅ›ci (0.0-1.0), wartoÅ›ci logiczne
- **Maskowaniem sekretÃ³w** - Klucze API, tokeny, hasÅ‚a sÄ… domyÅ›lnie ukryte
- **KopiÄ… zapasowÄ… konfiguracji** - Automatyczny backup `.env` do `config/env-history/` przed kaÅ¼dÄ… zmianÄ…
- **InformacjÄ… o restartach** - System wskazuje ktÃ³re usÅ‚ugi wymagajÄ… restartu po zmianie

#### DostÄ™pne sekcje parametrÃ³w:
1. **Tryb AI** - Tryb AI, endpoint LLM, klucze API, routing modeli
2. **Komendy** - Komendy start/stop dla Ollama i vLLM
3. **Hive** - Konfiguracja Redis, kolejki, timeouty
4. **Nexus** - siatka rozproszona, port, tokeny, heartbeat
5. **Zadania** - Zadania w tle (dokumentacja, porzÄ…dkowanie, konsolidacja pamiÄ™ci)
6. **Shadow** - Å›wiadomoÅ›Ä‡ pulpitu, progi pewnoÅ›ci, filtr prywatnoÅ›ci
7. **Ghost** - automatyzacja GUI, weryfikacja, opÃ³Åºnienia bezpieczeÅ„stwa
8. **Avatar** - interfejs audio, Whisper, TTS, VAD

#### BezpieczeÅ„stwo
- **BiaÅ‚a lista parametrÃ³w** - Tylko zdefiniowane parametry moÅ¼na edytowaÄ‡ przez UI
- **Walidacja typÃ³w i zakresÃ³w** - Sprawdzanie poprawnoÅ›ci wartoÅ›ci przed zapisem
- **Sprawdzanie zaleÅ¼noÅ›ci** - System nie pozwoli uruchomiÄ‡ usÅ‚ugi bez speÅ‚nienia wymagaÅ„ (np. Nexus wymaga dziaÅ‚ajÄ…cego backendu)
- **Historia zmian** - KaÅ¼da modyfikacja `.env` jest zapisywana z timestampem (zachowywanych ostatnie 50 backupÃ³w)

#### Przywracanie konfiguracji
Panel oferuje funkcjÄ™ przywracania `.env` z wczeÅ›niejszych backupÃ³w:
```bash
# Backupy znajdujÄ… siÄ™ w:
config/env-history/.env-YYYYMMDD-HHMMSS
```

> ğŸ’¡ **WskazÃ³wka**: Profile szybkie sÄ… idealne do przeÅ‚Ä…czania miÄ™dzy trybami pracy. UÅ¼yj `Light` podczas developmentu na laptopie, a `Full Stack` na stacji roboczej z GPU.

### ğŸ“Š Monitoring ZasobÃ³w

Venom oferuje narzÄ™dzia do szybkiej diagnostyki zuÅ¼ycia zasobÃ³w systemowych.

#### Zrzut systemu
```bash
# Generuje raport diagnostyczny (procesy, pamiÄ™Ä‡, CPU, status usÅ‚ug)
make monitor

# RÄ™czne uruchomienie
bash scripts/diagnostics/system_snapshot.sh
```

Raport zostanie zapisany w `logs/diag-YYYYMMDD-HHMMSS.txt` i zawiera:
- Uptime i load average
- ZuÅ¼ycie pamiÄ™ci (free -h, /proc/meminfo)
- Top 15 procesÃ³w (CPU i RAM)
- Status procesÃ³w Venom (uvicorn, Next.js, vLLM, Ollama)
- Status PID files i otwarte porty (8000, 3000, 8001, 11434)

**PrzykÅ‚ad uÅ¼ycia:**
```bash
# Przed rozpoczÄ™ciem pracy - sprawdÅº baseline
make monitor

# Po uruchomieniu usÅ‚ug - porÃ³wnaj zuÅ¼ycie
make api-dev
make web-dev
make monitor

# Po zakoÅ„czeniu - upewnij siÄ™ Å¼e wszystko zostaÅ‚o zatrzymane
make stop
make monitor
```

### ğŸ’¾ ZarzÄ…dzanie PamiÄ™ciÄ… WSL (Windows)

JeÅ›li uruchamiasz Venom w WSL (Windows Subsystem for Linux), moÅ¼esz napotkaÄ‡ problem z `vmmem` - procesem Windows, ktÃ³ry rezerwuje duÅ¼o RAM mimo niewielkiego zuÅ¼ycia po stronie Linuxa.

#### Sprawdzanie zuÅ¼ycia pamiÄ™ci
```bash
# PokaÅ¼ szczegÃ³Å‚owe statystyki pamiÄ™ci WSL
bash scripts/wsl/memory_check.sh
```

Skrypt wyÅ›wietli:
- Podsumowanie pamiÄ™ci (free -h)
- SzczegÃ³Å‚owe info z /proc/meminfo
- Top 10 procesÃ³w zuÅ¼ywajÄ…cych RAM
- ZuÅ¼ycie pamiÄ™ci przez poszczegÃ³lne komponenty Venom

#### Problem: vmmem zajmuje 20+ GB na Windows

**Symptom:** Task Manager w Windows pokazuje proces `vmmem` zajmujÄ…cy 20-30 GB RAM, mimo Å¼e `free -h` w WSL pokazuje tylko 3-4 GB.

**Przyczyna:** WSL nie zwraca pamiÄ™ci do Windows natychmiast. Cache i bufory sÄ… trzymane "na wszelki wypadek".

**RozwiÄ…zanie:**

1. **DoraÅºne:** Reset pamiÄ™ci WSL
   ```bash
   # Z poziomu WSL (zatrzyma wszystkie procesy Venom i wykona shutdown)
   bash scripts/wsl/reset_memory.sh

   # LUB z poziomu Windows (PowerShell/CMD)
   wsl --shutdown
   ```

2. **TrwaÅ‚e:** Limituj zuÅ¼ycie przez `.wslconfig`

   UtwÃ³rz plik `%USERPROFILE%\.wslconfig` (np. `C:\Users\TwojaNazwa\.wslconfig`):
   ```ini
   [wsl2]
   # Limit pamiÄ™ci dla WSL
   memory=12GB

   # Liczba procesorÃ³w
   processors=4

   # Limit swap
   swap=8GB
   ```

   DostÄ™pny przykÅ‚ad z komentarzami:
   ```bash
   # Zobacz peÅ‚nÄ… konfiguracjÄ™ z przykÅ‚adami
   cat scripts/wsl/wslconfig.example

   # Skopiuj do Windows (z poziomu WSL)
   cp scripts/wsl/wslconfig.example /mnt/c/Users/TwojaNazwa/.wslconfig
   ```

   Po zapisaniu `.wslconfig` wykonaj:
   ```powershell
   # Z poziomu Windows (PowerShell/CMD)
   wsl --shutdown
   ```

   NastÄ™pnie uruchom ponownie terminal WSL.

#### PrzykÅ‚adowe konfiguracje .wslconfig

**PC z 16 GB RAM (oszczÄ™dny):**
```ini
[wsl2]
memory=8GB
processors=4
swap=4GB
```

**PC z 32 GB RAM (zbalansowany):**
```ini
[wsl2]
memory=12GB
processors=6
swap=8GB
```

**Stacja robocza z 64 GB RAM (wydajnoÅ›Ä‡):**
```ini
[wsl2]
memory=32GB
processors=12
swap=16GB
```

#### Monitorowanie vmmem w Windows

1. OtwÃ³rz Task Manager (Ctrl+Shift+Esc)
2. ZakÅ‚adka "Details" lub "Processes"
3. ZnajdÅº proces "vmmem" - to jest pamiÄ™Ä‡ uÅ¼ywana przez WSL
4. PorÃ³wnaj z wynikami `free -h` w WSL

JeÅ›li rÃ³Å¼nica jest znaczna (>50%), rozwaÅ¼:
- Wykonanie `wsl --shutdown` aby zwolniÄ‡ cache
- Ustawienie limitÃ³w w `.wslconfig`
- UÅ¼ywanie profili Light (`make api` zamiast `make start-dev`)

### Uruchomienie

```bash
# Uruchom serwer
uvicorn venom_core.main:app --reload

# Lub uÅ¼yj make
make run
```

## ğŸ“– Dokumentacja

### Architektura i Wizja
- [Architektura systemu](docs/PL/VENOM_MASTER_VISION_V1.md)
- [Architektura backendu](docs/PL/BACKEND_ARCHITECTURE.md)
- [Architektura rozproszona (The Hive / Nexus)](docs/PL/THE_HIVE.md)
- [System rozpoznawania intencji](docs/PL/INTENT_RECOGNITION.md)
- [Silnik hybrydowy AI](docs/PL/HYBRID_AI_ENGINE.md)

### Agenci
- [**Indeks wszystkich agentÃ³w** (34 agenty)](docs/PL/AGENTS_INDEX.md) ğŸ“‹
- [The Architect - Planowanie](docs/PL/THE_ARCHITECT.md)
- [The Coder - Generowanie kodu](docs/PL/THE_CODER.md)
- [The Researcher - Wyszukiwanie wiedzy](docs/PL/THE_RESEARCHER.md)
- [The Chat - Asystent konwersacyjny](docs/PL/THE_CHAT.md)
- [The Strategist - Analiza zÅ‚oÅ¼onoÅ›ci](docs/PL/THE_STRATEGIST.md) *(PrzesuniÄ™te do v2.0)*
- [The Critic - Weryfikacja kodu](docs/PL/THE_CRITIC.md)
- [The Librarian - ZarzÄ…dzanie plikami](docs/PL/THE_LIBRARIAN.md)
- [The Integrator - Git & DevOps](docs/PL/THE_INTEGRATOR.md)
- [The Forge (Toolmaker) - Tworzenie narzÄ™dzi](docs/PL/THE_FORGE.md)

### Frontend i UI
- [Frontend Next.js](docs/PL/FRONTEND_NEXT_GUIDE.md)
- [Panel konfiguracji](docs/PL/CONFIG_PANEL.md)
- [Dashboard](docs/PL/DASHBOARD_GUIDE.md)

### NarzÄ™dzia i FunkcjonalnoÅ›ci
- [ZarzÄ…dzanie modelami](docs/PL/MODEL_MANAGEMENT.md)
- [Strojenie modelu LLM](docs/PL/MODEL_TUNING_GUIDE.md)
- [Flow Inspector](docs/PL/FLOW_INSPECTOR_GUIDE.md)
- [Dream Engine](docs/PL/DREAM_ENGINE_GUIDE.md) *(PrzesuniÄ™te do v2.0)*
- [Process Engine](docs/PL/PROCESS_ENGINE_CONCEPT.md) *(Planowany dla v2.0)*
- [Warstwa pamiÄ™ci](docs/PL/MEMORY_LAYER_GUIDE.md)
- [Google Search Grounding](docs/PL/GOOGLE_SEARCH_GROUNDING_INTEGRATION.md)

### DevOps i Deployment
- [Deployment (Next.js)](docs/PL/DEPLOYMENT_NEXT.md)
- [Integracje zewnÄ™trzne](docs/PL/EXTERNAL_INTEGRATIONS.md)
- [Guardian - BezpieczeÅ„stwo](docs/PL/GUARDIAN_GUIDE.md)
- [QA Delivery](docs/PL/QA_DELIVERY_GUIDE.md)

### WspÃ³Å‚praca
- [Przewodnik wspÃ³Å‚pracy](docs/PL/CONTRIBUTING.md)
- [Testowanie i wydajnoÅ›Ä‡](docs/PL/TESTING_CHAT_LATENCY.md)

## ğŸ§ª Testy

```bash
cd /home/ubuntu/venom
source .venv/bin/activate || true

# Szybki peÅ‚ny scenariusz (optymalne ustawienia dla naszego Å›rodowiska)
# pytest: heavy (-n 1), long (-n 2), light (-n 6)
pytest -n 1 $(cat config/pytest-groups/heavy.txt)
pytest -n 2 $(cat config/pytest-groups/long.txt)
pytest -n 6 $(cat config/pytest-groups/light.txt)

# Alternatywnie (skrypt):
./scripts/run-pytest-optimal.sh

# Alternatywnie (make):
make pytest

# Playwright E2E: latency (1 worker) + functional (4 workers)
npm --prefix web-next run test:e2e:preflight
npm --prefix web-next run test:e2e:latency
npm --prefix web-next run test:e2e:functional -- --workers=4
# SkÅ‚ad grupy functional: smoke + chat-mode-routing + streaming + chat-context-icons

# Alternatywnie (skrypt):
./scripts/run-e2e-optimal.sh

# Alternatywnie (make):
make e2e

# Tryb awaryjny (sÅ‚absze Å›rodowisko -> wszystko seryjnie)
pytest -n 1 $(cat config/pytest-groups/heavy.txt)
pytest -n 1 $(cat config/pytest-groups/long.txt)
pytest -n 1 $(cat config/pytest-groups/light.txt)
npm --prefix web-next run test:e2e:preflight
npm --prefix web-next run test:e2e:latency
npm --prefix web-next run test:e2e:functional -- --workers=1
```

## ğŸ”¬ Testy i benchmarki

PeÅ‚na instrukcja (kroki + oczekiwane wartoÅ›ci) jest w [`docs/PL/TESTING_CHAT_LATENCY.md`](docs/PL/TESTING_CHAT_LATENCY.md). NajwaÅ¼niejsze komendy:

### Backend (FastAPI / agenci)
- `pytest -q` â€” szybki test caÅ‚ego systemu.
- `pytest tests/test_researcher_agent.py` / `tests/test_architect_agent.py` â€” scenariusze agentÃ³w.
- `pytest tests/perf/test_chat_pipeline.py -m performance` â€” pomiar SSE (task_update â†’ task_finished) + batch rÃ³wnolegÅ‚y.
- `pytest --cov=venom_core --cov-report=html` â€” raport pokrycia.

### Frontend Next.js
- `npm --prefix web-next run lint`
- `npm --prefix web-next run build`
- `npm --prefix web-next run test:e2e` â€” Playwright na buildzie prod.
- Optymalnie (nasze Å›rodowisko): `test:e2e:latency` dziaÅ‚a na 1 workerze, `test:e2e:functional` na 4 workerach.
- W razie problemÃ³w uruchamiaj testy seryjnie (patrz â€œTryb awaryjnyâ€ powyÅ¼ej).

### Czas reakcji i wydajnoÅ›Ä‡ chatu
- `npm --prefix web-next run test:perf` â€” Playwright mierzÄ…cy latency Next Cockpit (raport HTML odkÅ‚ada siÄ™ do `test-results/perf-report`).
-  DostÄ™pne env-y: `PERF_NEXT_LATENCY_BUDGET` (domyÅ›lnie 15000ms) oraz `PERF_*_RESPONSE_TIMEOUT` jeÅ›li trzeba rozluÅºniÄ‡ limity na wolniejszych maszynach.
- `pytest tests/perf/test_chat_pipeline.py -m performance` â€” backendowy pipeline (czas do `task_finished` + batch).
- `./scripts/run-locust.sh` â€” start panelu Locusta (`http://127.0.0.1:8089`) i rÄ™czne obciÄ…Å¼enie API.
- `./scripts/archive-perf-results.sh` â€” zrzut `test-results/`, raportÃ³w Playwright/Locust do `perf-artifacts/<timestamp>/`.

> Wyniki testÃ³w NIE trafiajÄ… do repo (ignorujemy `**/test-results/`, `perf-artifacts/`, `playwright-report/`, itd.) â€“ dziÄ™ki temu przechowujesz je lokalnie bez ryzyka ujawnienia danych.

## ğŸ› ï¸ NarzÄ™dzia deweloperskie

### Bramy jakoÅ›ci i bezpieczeÅ„stwa

- **SonarCloud (bramka PR):** kaÅ¼dy pull request jest analizowany pod kÄ…tem bugÃ³w, podatnoÅ›ci, code smelli, duplikacji i utrzymywalnoÅ›ci.
- **Snyk (skan okresowy):** skan zaleÅ¼noÅ›ci i bezpieczeÅ„stwa kontenerÃ³w uruchamiany cyklicznie, aby wychwytywaÄ‡ nowe CVE.
- **CI Lite:** szybkie checki na kaÅ¼dym PR (lint + wybrane testy unit), Å¼eby skrÃ³ciÄ‡ pÄ™tlÄ™ informacji zwrotnej.

Co to oznacza dla contributorÃ³w i agentÃ³w:
- Pisz mniejsze, czytelne funkcje (unikaj wysokiej zÅ‚oÅ¼onoÅ›ci kognitywnej).
- Stosuj jawne typowanie i utrzymuj `mypy venom_core` na zielono.
- Usuwaj nieuÅ¼ywane importy/bloki i martwy kod.
- Traktuj ostrzeÅ¼enia `ruff`, `mypy` i Sonara jako blokery dla nowego kodu.

### Hooki pre-commit

```bash
# Instalacja
pip install pre-commit
pre-commit install

# Manualne uruchomienie
pre-commit run --all-files
```

### Linting i formatowanie

```bash
cd /home/ubuntu/venom
source .venv/bin/activate || true

# Ruff (linter + formatter)
ruff check . --fix
ruff format .

# isort (sortowanie importÃ³w)
isort .

# mypy (type checking)
mypy venom_core
```

NarzÄ™dzia korzystajÄ… z konfiguracji repo (`pyproject.toml`) i pomijajÄ… katalogi danych
takie jak `models/` i `models_cache/`.

## ğŸ“Š Statystyki projektu

- **Linie kodu:** 118,555 (linie niepuste; bez `docs/`, `node_modules/`, `logs/`, `data/`)
- **Liczba agentÃ³w:** 33 (moduÅ‚y `venom_core/agents/*`)
- **Liczba umiejÄ™tnoÅ›ci:** 19 wykonawczych (`venom_core/execution/skills/*`) + 4 pomocnicze (Memory/Voice/Whisper/Core)
- **Liczba testÃ³w:** 518 (pytest `def test_`) + 18 (Playwright `test(`)
- **Pokrycie testami:** 65%

## ğŸ¯ Mapa drogowa

### âœ… v1.0 (obecnie)
- [x] Warstwa Planowania (ArchitectAgent)
- [x] Ekspansja Wiedzy (ResearcherAgent + WebSearchSkill)
- [x] Integracja z Internetem
- [x] PamiÄ™Ä‡ dÅ‚ugoterminowa
- [x] Kompleksowe testy
- [x] **NOWE: Integracje zewnÄ™trzne (PlatformSkill)** ğŸ¤–
  - [x] Integracja GitHub (zgÅ‚oszenia Issue, pull requesty)
  - [x] Powiadomienia Discord/Slack
  - [x] Proces Issue â†’ PR

### ğŸš§ v1.2 (planowane)
- [ ] Odpytywanie w tle dla zgÅ‚oszeÅ„ GitHub Issues
- [ ] Panel dashboardu dla integracji zewnÄ™trznych
- [ ] Rekurencyjne streszczanie dÅ‚ugich dokumentÃ³w
- [ ] Cache wynikÃ³w wyszukiwania
- [ ] Walidacja i optymalizacja planu
- [ ] Lepsze odzyskiwanie po bÅ‚Ä™dach

### ğŸ”® v1.2 (w przyszÅ‚oÅ›ci)
- [ ] ObsÅ‚uga webhookÃ³w dla GitHub
- [ ] Integracja MS Teams
- [ ] Weryfikacja wieloÅºrÃ³dÅ‚owa
- [ ] Integracja Google Search API
- [ ] RÃ³wnolegÅ‚e wykonanie krokÃ³w planu
- [ ] Cache planÃ³w dla podobnych zadaÅ„
- [ ] Integracja GraphRAG

## ğŸ¤ WkÅ‚ad w projekt

Zapraszamy do wspÃ³Å‚pracy! Zobacz [CONTRIBUTING.md](docs/PL/CONTRIBUTING.md), aby dowiedzieÄ‡ siÄ™ jak zaczÄ…Ä‡.

### Proces wspÃ³Å‚pracy

1. Wykonaj fork repozytorium
2. StwÃ³rz gaÅ‚Ä…Åº dla funkcji (`git checkout -b feature/amazing-feature`)
3. ZrÃ³b commit zmian (`git commit -m 'feat: dodaj nowa funkcje'`)
4. Wypchnij gaÅ‚Ä…Åº (`git push origin feature/amazing-feature`)
5. OtwÃ³rz PR

### Konwencje

- **Kod i komentarze:** Polski lub angielski
- **WiadomoÅ›ci commitÃ³w:** Conventional Commits (feat, fix, docs, test, refactor)
- **Styl:** Black + Ruff + isort (automatyczne przez pre-commit)
- **Testy:** Wymagane dla nowych funkcjonalnoÅ›ci
- **Bramki jakoÅ›ci:** SonarCloud musi przejÅ›Ä‡ na PR; baza bezpieczeÅ„stwa jest monitorowana okresowymi skanami Snyk


## ğŸ‘¥ ZespÃ³Å‚

- **Lider rozwoju:** mpieniak01
- **Architektura:** Venom Core Team
- **WspÃ³Å‚autorzy:** [Lista kontrybutorÃ³w](https://github.com/mpieniak01/Venom/graphs/contributors)

## ğŸ™ PodziÄ™kowania

- Microsoft Semantic Kernel
- Microsoft AutoGen
- OpenAI / Anthropic / Google AI
- pytest
- SpoÅ‚ecznoÅ›Ä‡ Open Source

---

**Venom** - *Autonomiczny system agentÃ³w AI dla nastÄ™pnej generacji automatyzacji*

ğŸŒŸ JeÅ›li podoba Ci siÄ™ projekt, zostaw gwiazdkÄ™ na GitHub!

## ğŸ“ Licencja

Projekt jest udostÄ™pniany na licencji MIT. Zobacz plik [`LICENSE`](LICENSE), aby uzyskaÄ‡ wiÄ™cej informacji.

Copyright (c) 2025-2026 Maciej Pieniak

---
