"""Modu≈Ç: web_skill - Plugin Semantic Kernel do wyszukiwania w Internecie."""

from importlib import import_module
from typing import Annotated, Any, Optional

import httpx
from semantic_kernel.functions import kernel_function

from venom_core.config import SETTINGS
from venom_core.utils.helpers import extract_secret_value
from venom_core.utils.logger import get_logger

_trafilatura: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    import trafilatura as _trafilatura_module

    _trafilatura = _trafilatura_module
except Exception:  # pragma: no cover
    pass

trafilatura: Any = _trafilatura

_beautiful_soup_cls: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    from bs4 import BeautifulSoup as _BeautifulSoupClass

    _beautiful_soup_cls = _BeautifulSoupClass
except Exception:  # pragma: no cover
    pass

BeautifulSoup: Any = _beautiful_soup_cls

DDGS: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    ddgs_module = import_module("ddgs")
    DDGS = getattr(ddgs_module, "DDGS", None)
except Exception:  # pragma: no cover
    DDGS = None

if DDGS is None:  # pragma: no cover - fallback zale≈ºny od ≈õrodowiska
    try:
        duckduckgo_module = import_module("duckduckgo_search")
        DDGS = getattr(duckduckgo_module, "DDGS", None)
    except Exception:
        DDGS = None

logger = get_logger(__name__)

# Staramy siƒô opcjonalnie za≈Çadowaƒá TavilyClient aby testy mog≈Çy go mockowaƒá
_ImportedTavilyClient: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    _ImportedTavilyClient = getattr(import_module("tavily"), "TavilyClient", None)
except Exception:  # pragma: no cover
    pass

# Wystaw symbol na poziomie modu≈Çu (nawet je≈õli None), aby patchowanie by≈Ço mo≈ºliwe
TavilyClient = _ImportedTavilyClient

# Limity dla bezpiecze≈Ñstwa i wydajno≈õci
MAX_SEARCH_RESULTS = 5
MAX_SCRAPED_TEXT_LENGTH = 8000  # Maksymalna d≈Çugo≈õƒá tekstu ze strony (tokeny)
MAX_TOTAL_CONTEXT_LENGTH = 20000  # Maksymalna ≈ÇƒÖczna d≈Çugo≈õƒá dla wielu stron
MAX_CONTENT_PREVIEW_LENGTH = 200  # Maksymalna d≈Çugo≈õƒá podglƒÖdu opisu w wynikach
NO_TITLE_TEXT = "Brak tytu≈Çu"


class WebSearchSkill:
    """
    Skill do wyszukiwania informacji w Internecie.
    Pozwala agentom wyszukiwaƒá informacje i pobieraƒá tre≈õƒá ze stron WWW.
    Obs≈Çuguje Tavily AI Search (gdy skonfigurowany) lub DuckDuckGo (fallback).
    """

    def __init__(self):
        """Inicjalizacja WebSearchSkill."""
        # Sprawd≈∫ czy Tavily jest skonfigurowany
        self.tavily_client = None
        tavily_key = None

        # Pobierz AI_MODE dla strategii kosztowej
        self.ai_mode = getattr(SETTINGS, "AI_MODE", "LOCAL")

        if hasattr(SETTINGS, "TAVILY_API_KEY"):
            tavily_key = extract_secret_value(SETTINGS.TAVILY_API_KEY)

        if tavily_key:
            # Prefer dynamic import to respect test patching and runtime changes.
            tavily_cls = _get_tavily_client_class() or TavilyClient
        else:
            tavily_cls = None

        if tavily_key and tavily_cls is not None:
            try:
                self.tavily_client = tavily_cls(api_key=tavily_key)
                logger.info("WebSearchSkill zainicjalizowany z Tavily AI Search")
            except Exception as e:  # pragma: no cover - zale≈ºy od ≈õrodowiska
                logger.warning(
                    f"B≈ÇƒÖd inicjalizacji Tavily client: {e}. U≈ºywam DuckDuckGo jako fallback."
                )
        elif tavily_key:
            logger.warning(
                "tavily-python nie jest zainstalowane. U≈ºywam DuckDuckGo jako fallback."
            )
        else:
            logger.info(
                "WebSearchSkill zainicjalizowany z DuckDuckGo (brak TAVILY_API_KEY)"
            )

    def _truncate_scraped_text(self, text: str) -> str:
        if len(text) > MAX_SCRAPED_TEXT_LENGTH:
            return text[:MAX_SCRAPED_TEXT_LENGTH] + "\n\n[...tekst obciƒôty...]"
        return text

    def _scrape_with_trafilatura(self, url: str) -> str | None:
        if trafilatura is None:
            return None

        downloaded = trafilatura.fetch_url(url)
        if not downloaded:
            return None

        text = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=True,
            no_fallback=False,
        )
        if not text or not text.strip():
            return None

        text = self._truncate_scraped_text(text)
        logger.info(f"WebScrape: pobrano {len(text)} znak√≥w z {url} (trafilatura)")
        return f"Tre≈õƒá ze strony {url}:\n\n{text}"

    def _scrape_with_beautifulsoup(self, url: str) -> str:
        if BeautifulSoup is None:
            return (
                "‚ùå Brak biblioteki beautifulsoup4. "
                "Doinstaluj zale≈ºno≈õci aby u≈ºyƒá fallback scrape_text."
            )

        response = httpx.get(url, timeout=10, follow_redirects=True)
        response.raise_for_status()

        soup = BeautifulSoup(response.content, "html.parser")
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()

        text = soup.get_text(separator="\n", strip=True)
        text = "\n".join(line.strip() for line in text.split("\n") if line.strip())
        text = self._truncate_scraped_text(text)

        if not text.strip():
            return (
                f"Strona {url} nie zawiera wystarczajƒÖcej ilo≈õci tekstu "
                "lub jest niedostƒôpna."
            )

        logger.info(f"WebScrape: pobrano {len(text)} znak√≥w z {url} (BeautifulSoup)")
        return f"Tre≈õƒá ze strony {url}:\n\n{text}"

    @kernel_function(
        name="search",
        description="Wyszukuje informacje w Internecie u≈ºywajƒÖc Tavily AI Search (je≈õli skonfigurowany) lub DuckDuckGo. "
        "Zwraca listƒô tytu≈Ç√≥w, URL i kr√≥tkich opis√≥w znalezionych stron.",
    )
    def search(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        max_results: Annotated[
            int, "Maksymalna liczba wynik√≥w (domy≈õlnie 5)"
        ] = MAX_SEARCH_RESULTS,
    ) -> str:
        """
        Wyszukuje informacje w Internecie.

        Args:
            query: Zapytanie do wyszukiwarki
            max_results: Maksymalna liczba wynik√≥w

        Returns:
            Sformatowana lista wynik√≥w wyszukiwania
        """
        logger.info(
            f"WebSearch: szukanie '{query[:100]}...' (max {max_results} wynik√≥w)"
        )

        try:
            use_free_search = self._should_use_free_search()
            fallback_note = ""

            if self.tavily_client and not use_free_search:
                tavily_result = self._search_with_tavily(query, max_results)
                if tavily_result is not None:
                    return tavily_result
                fallback_note = "‚ö†Ô∏è Tavily niedostƒôpny, u≈ºyto DuckDuckGo\n\n"

            return self._search_with_duckduckgo(query, max_results, fallback_note)
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas wyszukiwania: {e}")
            return f"WystƒÖpi≈Ç b≈ÇƒÖd podczas wyszukiwania: {str(e)}"

    def _should_use_free_search(self) -> bool:
        force_free = getattr(SETTINGS, "LOW_COST_FORCE_DDG", True)
        return force_free and self.ai_mode in ("LOCAL", "ECO")

    def _search_with_tavily(self, query: str, max_results: int) -> str | None:
        try:
            assert self.tavily_client is not None
            response = self.tavily_client.search(
                query=query,
                max_results=max_results,
                include_answer=True,
                include_raw_content=False,
            )
        except Exception as tavily_error:
            logger.warning(f"B≈ÇƒÖd Tavily: {tavily_error}. Prze≈ÇƒÖczam na DuckDuckGo.")
            return None

        return self._format_tavily_response(query, response, max_results)

    def _format_tavily_response(
        self, query: str, response: dict[str, Any], max_results: int
    ) -> str:
        output = f"Znaleziono wyniki dla zapytania: '{query}'\n(≈∫r√≥d≈Ço: Tavily AI Search)\n\n"
        if response.get("answer"):
            output += f"üìã Podsumowanie AI:\n{response['answer']}\n\n"

        results = response.get("results", [])
        if not results:
            return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

        output += f"üîç ≈πr√≥d≈Ça ({len(results)}):\n\n"
        for i, result in enumerate(results[:max_results], 1):
            title = result.get("title", NO_TITLE_TEXT)
            url = result.get("url", "Brak URL")
            content = result.get("content", "Brak opisu")
            output += f"[{i}] {title}\nURL: {url}\n"
            output += f"Opis: {content[:MAX_CONTENT_PREVIEW_LENGTH]}...\n\n"

        logger.info(f"WebSearch (Tavily): znaleziono {len(results)} wynik√≥w")
        return output.strip()

    def _search_with_duckduckgo(
        self, query: str, max_results: int, fallback_note: str = ""
    ) -> str:
        with DDGS() as ddgs:
            results = list(ddgs.text(query, max_results=max_results))

        if not results:
            return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

        output = fallback_note
        output += f"Znaleziono {len(results)} wynik√≥w dla zapytania: '{query}'\n"
        output += "(≈∫r√≥d≈Ço: DuckDuckGo)\n\n"
        for i, result in enumerate(results, 1):
            title = result.get("title", NO_TITLE_TEXT)
            url = result.get("href", "Brak URL")
            snippet = result.get("body", "Brak opisu")
            output += f"[{i}] {title}\nURL: {url}\nOpis: {snippet}\n\n"

        logger.info(f"WebSearch (DuckDuckGo): znaleziono {len(results)} wynik√≥w")
        return output.strip()

    @kernel_function(
        name="scrape_text",
        description="Pobiera i oczyszcza tekst ze strony internetowej. Zwraca czysty tekst bez reklam i ≈õmieci HTML.",
    )
    def scrape_text(
        self,
        url: Annotated[str, "URL strony do pobrania"],
    ) -> str:
        """
        Pobiera i oczyszcza tekst ze strony WWW.

        Args:
            url: URL strony do pobrania

        Returns:
            Czysty tekst ze strony lub komunikat o b≈Çƒôdzie
        """
        logger.info(f"WebScrape: pobieranie tekstu z {url}")

        try:
            if trafilatura is None and BeautifulSoup is None:
                return (
                    "‚ùå Brak bibliotek do scrapowania (trafilatura/beautifulsoup4). "
                    "Doinstaluj zale≈ºno≈õci aby u≈ºyƒá scrape_text."
                )

            text = self._scrape_with_trafilatura(url)
            if text is not None:
                return text

            # Fallback do BeautifulSoup je≈õli trafilatura zawiod≈Ça
            logger.warning(
                f"Trafilatura nie zwr√≥ci≈Ça wynik√≥w dla {url}, pr√≥bujƒô BeautifulSoup"
            )
            return self._scrape_with_beautifulsoup(url)

        except httpx.TimeoutException:
            logger.error(f"Timeout podczas pobierania {url}")
            return f"Przekroczono limit czasu podczas pobierania {url}"
        except httpx.HTTPStatusError as e:
            logger.error(f"B≈ÇƒÖd HTTP podczas pobierania {url}: {e}")
            return f"B≈ÇƒÖd HTTP podczas pobierania {url}: {e.response.status_code}"
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas scrapowania {url}: {e}")
            return f"Nie uda≈Ço siƒô pobraƒá tre≈õci z {url}: {str(e)}"

    @kernel_function(
        name="search_and_scrape",
        description="Wyszukuje informacje w Internecie i automatycznie pobiera tre≈õƒá z najlepszych wynik√≥w. Zwraca skonsolidowanƒÖ wiedzƒô z wielu ≈∫r√≥de≈Ç.",
    )
    def search_and_scrape(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        num_sources: Annotated[int, "Liczba stron do pobrania (domy≈õlnie 3)"] = 3,
    ) -> str:
        """
        Wyszukuje i pobiera tre≈õƒá z najlepszych wynik√≥w.

        Args:
            query: Zapytanie do wyszukiwarki
            num_sources: Liczba stron do pobrania

        Returns:
            Skonsolidowana wiedza z wielu ≈∫r√≥de≈Ç
        """
        logger.info(
            f"WebSearchAndScrape: szukanie i pobieranie {num_sources} ≈∫r√≥de≈Ç dla '{query[:100]}...'"
        )

        try:
            # Ogranicz liczbƒô ≈∫r√≥de≈Ç
            num_sources = min(num_sources, MAX_SEARCH_RESULTS)

            # Wyszukaj
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=num_sources))

            if not results:
                return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

            # Pobierz tre≈õƒá z ka≈ºdego wyniku
            consolidated = f"Wyniki wyszukiwania dla: '{query}'\n\n"
            total_length = 0

            for i, result in enumerate(results, 1):
                url = result.get("href", "")
                title = result.get("title", NO_TITLE_TEXT)

                if not url:
                    continue

                consolidated += f"\n{'=' * 80}\n"
                consolidated += f"≈πR√ìD≈ÅO {i}: {title}\n"
                consolidated += f"URL: {url}\n"
                consolidated += f"{'=' * 80}\n\n"

                # Pobierz tre≈õƒá
                content = self.scrape_text(url)

                # Sprawd≈∫ limity
                if total_length + len(content) > MAX_TOTAL_CONTEXT_LENGTH:
                    logger.warning(
                        f"OsiƒÖgniƒôto limit ca≈Çkowitej d≈Çugo≈õci tekstu po {i} ≈∫r√≥d≈Çach"
                    )
                    consolidated += "\n[...osiƒÖgniƒôto limit d≈Çugo≈õci, pozosta≈Çe ≈∫r√≥d≈Ça pominiƒôte...]\n"
                    break

                consolidated += content + "\n\n"
                total_length += len(content)

            logger.info(
                f"WebSearchAndScrape: pobrano {total_length} znak√≥w z {len(results)} ≈∫r√≥de≈Ç"
            )
            return consolidated.strip()

        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas search_and_scrape: {e}")
            return f"WystƒÖpi≈Ç b≈ÇƒÖd podczas wyszukiwania i pobierania: {str(e)}"


def _get_tavily_client_class() -> Optional[type]:
    """≈Åaduje TavilyClient na ≈ºƒÖdanie."""

    try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
        module = import_module("tavily")
        return getattr(module, "TavilyClient")
    except Exception:  # pragma: no cover
        return None
