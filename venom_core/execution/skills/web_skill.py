"""Modu≈Ç: web_skill - Plugin Semantic Kernel do wyszukiwania w Internecie."""

from importlib import import_module
from typing import Annotated, Any, Optional

import httpx
from semantic_kernel.functions import kernel_function

from venom_core.config import SETTINGS
from venom_core.utils.helpers import extract_secret_value
from venom_core.utils.logger import get_logger

_trafilatura: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    import trafilatura as _trafilatura_module

    _trafilatura = _trafilatura_module
except Exception:  # pragma: no cover
    pass

trafilatura: Any = _trafilatura

_beautiful_soup_cls: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    from bs4 import BeautifulSoup as _BeautifulSoupClass

    _beautiful_soup_cls = _BeautifulSoupClass
except Exception:  # pragma: no cover
    pass

BeautifulSoup: Any = _beautiful_soup_cls

_DDGS: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    from ddgs import DDGS as _DDGS
except Exception:  # pragma: no cover
    _DDGS = None

if _DDGS is None:  # pragma: no cover - fallback zale≈ºny od ≈õrodowiska
    try:
        from duckduckgo_search import DDGS as _DDGS
    except Exception:
        _DDGS = None

DDGS: Any = _DDGS

logger = get_logger(__name__)

# Staramy siƒô opcjonalnie za≈Çadowaƒá TavilyClient aby testy mog≈Çy go mockowaƒá
_ImportedTavilyClient: Any = None
try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
    _ImportedTavilyClient = getattr(import_module("tavily"), "TavilyClient", None)
except Exception:  # pragma: no cover
    pass

# Wystaw symbol na poziomie modu≈Çu (nawet je≈õli None), aby patchowanie by≈Ço mo≈ºliwe
TavilyClient = _ImportedTavilyClient

# Limity dla bezpiecze≈Ñstwa i wydajno≈õci
MAX_SEARCH_RESULTS = 5
MAX_SCRAPED_TEXT_LENGTH = 8000  # Maksymalna d≈Çugo≈õƒá tekstu ze strony (tokeny)
MAX_TOTAL_CONTEXT_LENGTH = 20000  # Maksymalna ≈ÇƒÖczna d≈Çugo≈õƒá dla wielu stron
MAX_CONTENT_PREVIEW_LENGTH = 200  # Maksymalna d≈Çugo≈õƒá podglƒÖdu opisu w wynikach


class WebSearchSkill:
    """
    Skill do wyszukiwania informacji w Internecie.
    Pozwala agentom wyszukiwaƒá informacje i pobieraƒá tre≈õƒá ze stron WWW.
    Obs≈Çuguje Tavily AI Search (gdy skonfigurowany) lub DuckDuckGo (fallback).
    """

    def __init__(self):
        """Inicjalizacja WebSearchSkill."""
        # Sprawd≈∫ czy Tavily jest skonfigurowany
        self.tavily_client = None
        tavily_key = None

        # Pobierz AI_MODE dla strategii kosztowej
        self.ai_mode = getattr(SETTINGS, "AI_MODE", "LOCAL")

        if hasattr(SETTINGS, "TAVILY_API_KEY"):
            tavily_key = extract_secret_value(SETTINGS.TAVILY_API_KEY)

        if tavily_key:
            # Prefer dynamic import to respect test patching and runtime changes.
            tavily_cls = _get_tavily_client_class() or TavilyClient
        else:
            tavily_cls = None

        if tavily_key and tavily_cls is not None:
            try:
                self.tavily_client = tavily_cls(api_key=tavily_key)
                logger.info("WebSearchSkill zainicjalizowany z Tavily AI Search")
            except Exception as e:  # pragma: no cover - zale≈ºy od ≈õrodowiska
                logger.warning(
                    f"B≈ÇƒÖd inicjalizacji Tavily client: {e}. U≈ºywam DuckDuckGo jako fallback."
                )
        elif tavily_key:
            logger.warning(
                "tavily-python nie jest zainstalowane. U≈ºywam DuckDuckGo jako fallback."
            )
        else:
            logger.info(
                "WebSearchSkill zainicjalizowany z DuckDuckGo (brak TAVILY_API_KEY)"
            )

    @kernel_function(
        name="search",
        description="Wyszukuje informacje w Internecie u≈ºywajƒÖc Tavily AI Search (je≈õli skonfigurowany) lub DuckDuckGo. "
        "Zwraca listƒô tytu≈Ç√≥w, URL i kr√≥tkich opis√≥w znalezionych stron.",
    )
    def search(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        max_results: Annotated[
            int, "Maksymalna liczba wynik√≥w (domy≈õlnie 5)"
        ] = MAX_SEARCH_RESULTS,
    ) -> str:
        """
        Wyszukuje informacje w Internecie.

        Args:
            query: Zapytanie do wyszukiwarki
            max_results: Maksymalna liczba wynik√≥w

        Returns:
            Sformatowana lista wynik√≥w wyszukiwania
        """
        logger.info(
            f"WebSearch: szukanie '{query[:100]}...' (max {max_results} wynik√≥w)"
        )

        try:
            # LOW-COST ROUTING: W trybie LOCAL/ECO zawsze wymuszamy darmowe
            # ≈∫r√≥d≈Ça (DuckDuckGo) aby uniknƒÖƒá p≈Çatnych zapyta≈Ñ.
            force_free = getattr(SETTINGS, "LOW_COST_FORCE_DDG", True)
            use_free_search = force_free and self.ai_mode in ("LOCAL", "ECO")

            # U≈ºyj Tavily je≈õli dostƒôpny i nie jeste≈õmy w trybie LOCAL/ECO
            if self.tavily_client and not use_free_search:
                try:
                    response = self.tavily_client.search(
                        query=query,
                        max_results=max_results,
                        include_answer=True,
                        include_raw_content=False,
                    )

                    # Formatuj wyniki Tavily
                    output = f"Znaleziono wyniki dla zapytania: '{query}'\n"
                    output += "(≈∫r√≥d≈Ço: Tavily AI Search)\n\n"

                    # Dodaj AI-generated answer je≈õli dostƒôpny
                    if response.get("answer"):
                        output += f"üìã Podsumowanie AI:\n{response['answer']}\n\n"

                    results = response.get("results", [])
                    if not results:
                        return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

                    output += f"üîç ≈πr√≥d≈Ça ({len(results)}):\n\n"
                    for i, result in enumerate(results[:max_results], 1):
                        title = result.get("title", "Brak tytu≈Çu")
                        url = result.get("url", "Brak URL")
                        content = result.get("content", "Brak opisu")

                        output += f"[{i}] {title}\n"
                        output += f"URL: {url}\n"
                        # U≈ºyj sta≈Çej zamiast hardcoded warto≈õci
                        output += f"Opis: {content[:MAX_CONTENT_PREVIEW_LENGTH]}...\n\n"

                    logger.info(
                        f"WebSearch (Tavily): znaleziono {len(results)} wynik√≥w"
                    )
                    return output.strip()

                except Exception as tavily_error:
                    logger.warning(
                        f"B≈ÇƒÖd Tavily: {tavily_error}. Prze≈ÇƒÖczam na DuckDuckGo."
                    )
                    # Informuj agenta o fallback
                    fallback_note = "‚ö†Ô∏è Tavily niedostƒôpny, u≈ºyto DuckDuckGo\n\n"
                    # Fallback do DuckDuckGo poni≈ºej

            # Fallback: U≈ºyj DuckDuckGo
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=max_results))

            if not results:
                return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

            # Formatuj wyniki DuckDuckGo
            output = ""
            # Dodaj notatkƒô o fallback je≈õli by≈Ça pr√≥ba u≈ºycia Tavily
            if self.tavily_client and "fallback_note" in locals():
                output += fallback_note
            output += f"Znaleziono {len(results)} wynik√≥w dla zapytania: '{query}'\n"
            output += "(≈∫r√≥d≈Ço: DuckDuckGo)\n\n"
            for i, result in enumerate(results, 1):
                title = result.get("title", "Brak tytu≈Çu")
                url = result.get("href", "Brak URL")
                snippet = result.get("body", "Brak opisu")

                output += f"[{i}] {title}\n"
                output += f"URL: {url}\n"
                output += f"Opis: {snippet}\n\n"

            logger.info(f"WebSearch (DuckDuckGo): znaleziono {len(results)} wynik√≥w")
            return output.strip()

        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas wyszukiwania: {e}")
            return f"WystƒÖpi≈Ç b≈ÇƒÖd podczas wyszukiwania: {str(e)}"

    @kernel_function(
        name="scrape_text",
        description="Pobiera i oczyszcza tekst ze strony internetowej. Zwraca czysty tekst bez reklam i ≈õmieci HTML.",
    )
    def scrape_text(
        self,
        url: Annotated[str, "URL strony do pobrania"],
    ) -> str:
        """
        Pobiera i oczyszcza tekst ze strony WWW.

        Args:
            url: URL strony do pobrania

        Returns:
            Czysty tekst ze strony lub komunikat o b≈Çƒôdzie
        """
        logger.info(f"WebScrape: pobieranie tekstu z {url}")

        try:
            if trafilatura is None and BeautifulSoup is None:
                return (
                    "‚ùå Brak bibliotek do scrapowania (trafilatura/beautifulsoup4). "
                    "Doinstaluj zale≈ºno≈õci aby u≈ºyƒá scrape_text."
                )

            # Najpierw spr√≥buj trafilatura (lepsze czyszczenie)
            downloaded = trafilatura.fetch_url(url) if trafilatura is not None else None
            if downloaded:
                text = trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                )

                if text and text.strip():
                    # Ogranicz d≈Çugo≈õƒá
                    if len(text) > MAX_SCRAPED_TEXT_LENGTH:
                        text = (
                            text[:MAX_SCRAPED_TEXT_LENGTH] + "\n\n[...tekst obciƒôty...]"
                        )

                    logger.info(
                        f"WebScrape: pobrano {len(text)} znak√≥w z {url} (trafilatura)"
                    )
                    return f"Tre≈õƒá ze strony {url}:\n\n{text}"

            # Fallback do BeautifulSoup je≈õli trafilatura zawiod≈Ça
            logger.warning(
                f"Trafilatura nie zwr√≥ci≈Ça wynik√≥w dla {url}, pr√≥bujƒô BeautifulSoup"
            )

            if BeautifulSoup is None:
                return (
                    "‚ùå Brak biblioteki beautifulsoup4. "
                    "Doinstaluj zale≈ºno≈õci aby u≈ºyƒá fallback scrape_text."
                )

            response = httpx.get(url, timeout=10, follow_redirects=True)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # Usu≈Ñ skrypty, style, itp.
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            # Pobierz tekst
            text = soup.get_text(separator="\n", strip=True)

            # Usu≈Ñ puste linie
            lines = [line.strip() for line in text.split("\n") if line.strip()]
            text = "\n".join(lines)

            # Ogranicz d≈Çugo≈õƒá
            if len(text) > MAX_SCRAPED_TEXT_LENGTH:
                text = text[:MAX_SCRAPED_TEXT_LENGTH] + "\n\n[...tekst obciƒôty...]"

            if not text.strip():
                return f"Strona {url} nie zawiera wystarczajƒÖcej ilo≈õci tekstu lub jest niedostƒôpna."

            logger.info(
                f"WebScrape: pobrano {len(text)} znak√≥w z {url} (BeautifulSoup)"
            )
            return f"Tre≈õƒá ze strony {url}:\n\n{text}"

        except httpx.TimeoutException:
            logger.error(f"Timeout podczas pobierania {url}")
            return f"Przekroczono limit czasu podczas pobierania {url}"
        except httpx.HTTPStatusError as e:
            logger.error(f"B≈ÇƒÖd HTTP podczas pobierania {url}: {e}")
            return f"B≈ÇƒÖd HTTP podczas pobierania {url}: {e.response.status_code}"
        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas scrapowania {url}: {e}")
            return f"Nie uda≈Ço siƒô pobraƒá tre≈õci z {url}: {str(e)}"

    @kernel_function(
        name="search_and_scrape",
        description="Wyszukuje informacje w Internecie i automatycznie pobiera tre≈õƒá z najlepszych wynik√≥w. Zwraca skonsolidowanƒÖ wiedzƒô z wielu ≈∫r√≥de≈Ç.",
    )
    def search_and_scrape(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        num_sources: Annotated[int, "Liczba stron do pobrania (domy≈õlnie 3)"] = 3,
    ) -> str:
        """
        Wyszukuje i pobiera tre≈õƒá z najlepszych wynik√≥w.

        Args:
            query: Zapytanie do wyszukiwarki
            num_sources: Liczba stron do pobrania

        Returns:
            Skonsolidowana wiedza z wielu ≈∫r√≥de≈Ç
        """
        logger.info(
            f"WebSearchAndScrape: szukanie i pobieranie {num_sources} ≈∫r√≥de≈Ç dla '{query[:100]}...'"
        )

        try:
            # Ogranicz liczbƒô ≈∫r√≥de≈Ç
            num_sources = min(num_sources, MAX_SEARCH_RESULTS)

            # Wyszukaj
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=num_sources))

            if not results:
                return f"Nie znaleziono wynik√≥w dla zapytania: {query}"

            # Pobierz tre≈õƒá z ka≈ºdego wyniku
            consolidated = f"Wyniki wyszukiwania dla: '{query}'\n\n"
            total_length = 0

            for i, result in enumerate(results, 1):
                url = result.get("href", "")
                title = result.get("title", "Brak tytu≈Çu")

                if not url:
                    continue

                consolidated += f"\n{'=' * 80}\n"
                consolidated += f"≈πR√ìD≈ÅO {i}: {title}\n"
                consolidated += f"URL: {url}\n"
                consolidated += f"{'=' * 80}\n\n"

                # Pobierz tre≈õƒá
                content = self.scrape_text(url)

                # Sprawd≈∫ limity
                if total_length + len(content) > MAX_TOTAL_CONTEXT_LENGTH:
                    logger.warning(
                        f"OsiƒÖgniƒôto limit ca≈Çkowitej d≈Çugo≈õci tekstu po {i} ≈∫r√≥d≈Çach"
                    )
                    consolidated += "\n[...osiƒÖgniƒôto limit d≈Çugo≈õci, pozosta≈Çe ≈∫r√≥d≈Ça pominiƒôte...]\n"
                    break

                consolidated += content + "\n\n"
                total_length += len(content)

            logger.info(
                f"WebSearchAndScrape: pobrano {total_length} znak√≥w z {len(results)} ≈∫r√≥de≈Ç"
            )
            return consolidated.strip()

        except Exception as e:
            logger.error(f"B≈ÇƒÖd podczas search_and_scrape: {e}")
            return f"WystƒÖpi≈Ç b≈ÇƒÖd podczas wyszukiwania i pobierania: {str(e)}"


def _get_tavily_client_class() -> Optional[type]:
    """≈Åaduje TavilyClient na ≈ºƒÖdanie."""

    try:  # pragma: no cover - zale≈ºne od ≈õrodowiska
        module = import_module("tavily")
        return getattr(module, "TavilyClient")
    except Exception:  # pragma: no cover
        return None
