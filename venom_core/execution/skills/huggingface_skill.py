"""ModuÅ‚: huggingface_skill - Skill do wyszukiwania modeli i datasets na Hugging Face."""

from pathlib import Path
from typing import Annotated, Optional

from huggingface_hub import HfApi, hf_hub_download
from huggingface_hub.utils import RepositoryNotFoundError
from semantic_kernel.functions import kernel_function

from venom_core.utils.logger import get_logger

logger = get_logger(__name__)

# Limity dla bezpieczeÅ„stwa i wydajnoÅ›ci
MAX_MODELS_RESULTS = 5
MAX_DATASETS_RESULTS = 5
MAX_MODEL_CARD_LENGTH = 8000
# MnoÅ¼nik dla filtrowania modeli ONNX/GGUF
# Pobieramy wiÄ™cej wynikÃ³w niÅ¼ potrzeba, aby mÃ³c wyfiltrowaÄ‡ i preferowaÄ‡ ONNX/GGUF
FILTER_MULTIPLIER = 3


class HuggingFaceSkill:
    """
    Skill do wyszukiwania modeli AI i zbiorÃ³w danych na Hugging Face.
    Pozwala agentom znajdowaÄ‡ odpowiednie modele, sprawdzaÄ‡ ich parametry i wymagania.
    """

    def __init__(self, hf_token: Optional[str] = None):
        """
        Inicjalizacja HuggingFaceSkill.

        Args:
            hf_token: Token Hugging Face API (opcjonalny, dla prywatnych modeli).
        """
        self.api = HfApi(token=hf_token)
        logger.info("HuggingFaceSkill zainicjalizowany")

    @kernel_function(
        name="search_models",
        description="Wyszukuje modele AI na Hugging Face wedÅ‚ug zadania i zapytania. Preferuje modele ONNX i GGUF (kompatybilne lokalnie). UÅ¼yj gdy uÅ¼ytkownik szuka modelu do konkretnego zadania NLP/Vision.",
    )
    def search_models(
        self,
        task: Annotated[
            str,
            "Typ zadania (np. 'text-classification', 'question-answering', 'image-classification', 'text-generation')",
        ] = "",
        query: Annotated[str, "Zapytanie tekstowe (np. 'sentiment', 'polish')"] = "",
        sort: Annotated[
            str, "Sortowanie: 'downloads', 'likes', 'trending'"
        ] = "downloads",
    ) -> str:
        """
        Wyszukuje modele na Hugging Face.

        Args:
            task: Typ zadania ML
            query: Zapytanie tekstowe
            sort: Kryterium sortowania

        Returns:
            Sformatowana lista TOP 5 modeli
        """
        logger.info(
            f"HuggingFaceSkill: search_models (task={task}, query={query}, sort={sort})"
        )

        try:
            # Parametry wyszukiwania
            search_params = {
                "limit": MAX_MODELS_RESULTS * FILTER_MULTIPLIER,
                "sort": sort,
            }

            # Dodaj task jeÅ›li podany
            if task:
                search_params["filter"] = task

            # Dodaj query jeÅ›li podany
            if query:
                search_params["search"] = query

            # Wyszukaj modele
            models = list(self.api.list_models(**search_params))

            if not models:
                return f"Nie znaleziono modeli dla: task={task}, query={query}"

            # Preferuj modele z ONNX lub GGUF (lekkie, lokalne)
            onnx_models = []
            gguf_models = []
            other_models = []

            for model in models:
                tags_lower = [tag.lower() for tag in (model.tags or [])]
                model_id_lower = model.id.lower()

                # Klasyfikuj i zapisz typ kompatybilnoÅ›ci
                if "onnx" in tags_lower or "onnx" in model_id_lower:
                    model._venom_compat = "âœ… ONNX (lokalne uruchamianie)"
                    onnx_models.append(model)
                elif "gguf" in tags_lower or "gguf" in model_id_lower:
                    model._venom_compat = "âœ… GGUF (lokalne uruchamianie)"
                    gguf_models.append(model)
                else:
                    model._venom_compat = "âš ï¸ Standard (wymaga GPU/transformers)"
                    other_models.append(model)

            # Preferuj kolejnoÅ›Ä‡: ONNX > GGUF > inne
            prioritized_models = (
                onnx_models[:MAX_MODELS_RESULTS]
                + gguf_models[: MAX_MODELS_RESULTS - len(onnx_models)]
                + other_models[
                    : MAX_MODELS_RESULTS - len(onnx_models) - len(gguf_models)
                ]
            )

            # Ogranicz do TOP 5
            results = []
            for i, model in enumerate(prioritized_models[:MAX_MODELS_RESULTS], 1):
                model_info = {
                    "rank": i,
                    "id": model.id,
                    "task": getattr(model, "pipeline_tag", "Nieznane"),
                    "downloads": getattr(model, "downloads", 0),
                    "likes": getattr(model, "likes", 0),
                    "url": f"https://huggingface.co/{model.id}",
                    "tags": ", ".join(model.tags[:5]) if model.tags else "Brak tagÃ³w",
                    "compatibility": model._venom_compat,
                }

                results.append(model_info)

            if not results:
                return f"Nie znaleziono modeli dla: task={task}, query={query}"

            # Formatuj wyniki
            output = f"ğŸ¤— TOP {len(results)} modeli Hugging Face\n"
            if task:
                output += f"ğŸ“‹ Zadanie: {task}\n"
            if query:
                output += f"ğŸ” Zapytanie: {query}\n"
            output += "\n"

            for r in results:
                output += f"[{r['rank']}] {r['id']}\n"
                output += (
                    f"ğŸ“Š Pobrania: {r['downloads']:,} | â¤ï¸ Polubienia: {r['likes']}\n"
                )
                output += f"ğŸ¯ Zadanie: {r['task']}\n"
                output += f"{r['compatibility']}\n"
                output += f"ğŸ·ï¸ Tagi: {r['tags']}\n"
                output += f"ğŸ”— URL: {r['url']}\n\n"

            logger.info(f"HuggingFaceSkill: znaleziono {len(results)} modeli")
            return output.strip()

        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas wyszukiwania modeli: {e}")
            return f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d: {str(e)}"

    @kernel_function(
        name="get_model_card",
        description="Pobiera szczegÃ³Å‚owy opis modelu (Model Card) z Hugging Face, zawierajÄ…cy informacje o architekturze, wymaganiach sprzÄ™towych, licencji i przykÅ‚adach uÅ¼ycia.",
    )
    def get_model_card(
        self,
        model_id: Annotated[str, "ID modelu (np. 'distilbert-base-uncased')"],
    ) -> str:
        """
        Pobiera Model Card (opis modelu).

        Args:
            model_id: ID modelu na Hugging Face

        Returns:
            Opis modelu lub komunikat o bÅ‚Ä™dzie
        """
        logger.info(f"HuggingFaceSkill: get_model_card dla {model_id}")

        try:
            # Pobierz informacje o modelu
            model_info = self.api.model_info(model_id)

            # Pobierz README (Model Card)
            try:
                # UÅ¼ywamy card_data do pobrania treÅ›ci
                card_content: str = (
                    str(model_info.card_data) if model_info.card_data else ""
                )

                # JeÅ›li card_data jest puste, sprÃ³buj pobraÄ‡ bezpoÅ›rednio
                if not card_content:
                    # Pobierz plik README.md
                    try:
                        readme_path = hf_hub_download(
                            repo_id=model_id,
                            filename="README.md",
                            repo_type="model",
                        )
                        # UÅ¼yj pathlib do bezpiecznego czytania pliku
                        card_content = Path(readme_path).read_text(encoding="utf-8")
                    except (FileNotFoundError, PermissionError, OSError) as e:
                        logger.debug(f"Nie moÅ¼na pobraÄ‡ README dla {model_id}: {e}")
                        card_content = "Brak dostÄ™pnego Model Card"
                else:
                    # card_content juÅ¼ jest stringiem
                    card_content = str(card_content)

            except Exception as e:
                logger.warning(f"Nie udaÅ‚o siÄ™ pobraÄ‡ Model Card: {e}")
                card_content = "Brak dostÄ™pnego Model Card"

            # Ogranicz dÅ‚ugoÅ›Ä‡
            if (
                isinstance(card_content, str)
                and len(card_content) > MAX_MODEL_CARD_LENGTH
            ):
                card_content = (
                    card_content[:MAX_MODEL_CARD_LENGTH]
                    + "\n\n[...Model Card obciÄ™ty...]"
                )

            # Formatuj wynik
            output = f"ğŸ¤— Model Card: {model_id}\n"
            output += f"ğŸ”— URL: https://huggingface.co/{model_id}\n"
            output += f"ğŸ“Š Pobrania: {getattr(model_info, 'downloads', 0):,}\n"
            output += f"â¤ï¸ Polubienia: {getattr(model_info, 'likes', 0)}\n"
            output += f"ğŸ¯ Zadanie: {getattr(model_info, 'pipeline_tag', 'Nieznane')}\n"

            # Licencja
            license_info = getattr(model_info, "card_data", {})
            if license_info and hasattr(license_info, "get"):
                license_name = license_info.get("license", "Nieznana")
            else:
                license_name = "Nieznana"
            output += f"ğŸ“œ Licencja: {license_name}\n"

            # Tagi
            if model_info.tags:
                output += f"ğŸ·ï¸ Tagi: {', '.join(model_info.tags[:10])}\n"

            output += f"\n{'=' * 80}\n\n"
            output += str(card_content)

            logger.info(f"HuggingFaceSkill: pobrano Model Card dla {model_id}")
            return output

        except RepositoryNotFoundError:
            logger.warning(f"Model nie znaleziony: {model_id}")
            return f"âŒ Model nie znaleziony: {model_id}"
        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas pobierania Model Card: {e}")
            return f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d: {str(e)}"

    @kernel_function(
        name="search_datasets",
        description="Wyszukuje zbiory danych (datasets) na Hugging Face wedÅ‚ug zapytania. UÅ¼yj gdy uÅ¼ytkownik szuka danych do treningu lub testowania modeli.",
    )
    def search_datasets(
        self,
        query: Annotated[str, "Zapytanie tekstowe (np. 'sentiment', 'polish', 'qa')"],
    ) -> str:
        """
        Wyszukuje zbiory danych na Hugging Face.

        Args:
            query: Zapytanie tekstowe

        Returns:
            Sformatowana lista zbiorÃ³w danych
        """
        logger.info(f"HuggingFaceSkill: search_datasets dla '{query}'")

        try:
            # Wyszukaj datasets
            datasets = list(
                self.api.list_datasets(
                    search=query,
                    limit=MAX_DATASETS_RESULTS,
                    sort="downloads",
                )
            )

            if not datasets:
                return f"Nie znaleziono zbiorÃ³w danych dla zapytania: {query}"

            # Formatuj wyniki
            results = []
            for i, dataset in enumerate(datasets, 1):
                dataset_info = {
                    "rank": i,
                    "id": dataset.id,
                    "downloads": getattr(dataset, "downloads", 0),
                    "likes": getattr(dataset, "likes", 0),
                    "url": f"https://huggingface.co/datasets/{dataset.id}",
                    "tags": (
                        ", ".join(dataset.tags[:5]) if dataset.tags else "Brak tagÃ³w"
                    ),
                }
                results.append(dataset_info)

            output = f"ğŸ—‚ï¸ TOP {len(results)} zbiorÃ³w danych dla: '{query}'\n\n"
            for r in results:
                output += f"[{r['rank']}] {r['id']}\n"
                output += (
                    f"ğŸ“Š Pobrania: {r['downloads']:,} | â¤ï¸ Polubienia: {r['likes']}\n"
                )
                output += f"ğŸ·ï¸ Tagi: {r['tags']}\n"
                output += f"ğŸ”— URL: {r['url']}\n\n"

            logger.info(f"HuggingFaceSkill: znaleziono {len(results)} zbiorÃ³w danych")
            return output.strip()

        except Exception as e:
            logger.error(f"BÅ‚Ä…d podczas wyszukiwania zbiorÃ³w danych: {e}")
            return f"âŒ WystÄ…piÅ‚ bÅ‚Ä…d: {str(e)}"
