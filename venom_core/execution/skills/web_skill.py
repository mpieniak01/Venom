"""Modu: web_skill - Plugin Semantic Kernel do wyszukiwania w Internecie."""

from importlib import import_module
from typing import Annotated, Any, Optional

import httpx
import trafilatura
from bs4 import BeautifulSoup
from semantic_kernel.functions import kernel_function

from venom_core.config import SETTINGS
from venom_core.utils.helpers import extract_secret_value
from venom_core.utils.logger import get_logger

_DDGS: Any = None
try:  # pragma: no cover - zale偶ne od rodowiska
    from ddgs import DDGS as _DDGS
except Exception:  # pragma: no cover
    _DDGS = None

if _DDGS is None:  # pragma: no cover - fallback zale偶ny od rodowiska
    try:
        from duckduckgo_search import DDGS as _DDGS
    except Exception:
        _DDGS = None

DDGS: Any = _DDGS

logger = get_logger(__name__)

# Staramy si opcjonalnie zaadowa TavilyClient aby testy mogy go mockowa
try:  # pragma: no cover - zale偶ne od rodowiska
    from tavily import (  # type: ignore[import-untyped] # isort: skip
        TavilyClient as _ImportedTavilyClient,
    )
except Exception:  # pragma: no cover
    _ImportedTavilyClient = None

# Wystaw symbol na poziomie moduu (nawet jeli None), aby patchowanie byo mo偶liwe
TavilyClient = _ImportedTavilyClient

# Limity dla bezpieczestwa i wydajnoci
MAX_SEARCH_RESULTS = 5
MAX_SCRAPED_TEXT_LENGTH = 8000  # Maksymalna dugo tekstu ze strony (tokeny)
MAX_TOTAL_CONTEXT_LENGTH = 20000  # Maksymalna czna dugo dla wielu stron
MAX_CONTENT_PREVIEW_LENGTH = 200  # Maksymalna dugo podgldu opisu w wynikach


class WebSearchSkill:
    """
    Skill do wyszukiwania informacji w Internecie.
    Pozwala agentom wyszukiwa informacje i pobiera tre ze stron WWW.
    Obsuguje Tavily AI Search (gdy skonfigurowany) lub DuckDuckGo (fallback).
    """

    def __init__(self):
        """Inicjalizacja WebSearchSkill."""
        # Sprawd藕 czy Tavily jest skonfigurowany
        self.tavily_client = None
        tavily_key = None

        # Pobierz AI_MODE dla strategii kosztowej
        self.ai_mode = getattr(SETTINGS, "AI_MODE", "LOCAL")

        if hasattr(SETTINGS, "TAVILY_API_KEY"):
            tavily_key = extract_secret_value(SETTINGS.TAVILY_API_KEY)

        if tavily_key:
            # Prefer dynamic import to respect test patching and runtime changes.
            tavily_cls = _get_tavily_client_class() or TavilyClient
        else:
            tavily_cls = None

        if tavily_key and tavily_cls is not None:
            try:
                self.tavily_client = tavily_cls(api_key=tavily_key)
                logger.info("WebSearchSkill zainicjalizowany z Tavily AI Search")
            except Exception as e:  # pragma: no cover - zale偶y od rodowiska
                logger.warning(
                    f"Bd inicjalizacji Tavily client: {e}. U偶ywam DuckDuckGo jako fallback."
                )
        elif tavily_key:
            logger.warning(
                "tavily-python nie jest zainstalowane. U偶ywam DuckDuckGo jako fallback."
            )
        else:
            logger.info(
                "WebSearchSkill zainicjalizowany z DuckDuckGo (brak TAVILY_API_KEY)"
            )

    @kernel_function(
        name="search",
        description="Wyszukuje informacje w Internecie u偶ywajc Tavily AI Search (jeli skonfigurowany) lub DuckDuckGo. "
        "Zwraca list tytu贸w, URL i kr贸tkich opis贸w znalezionych stron.",
    )
    def search(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        max_results: Annotated[
            int, "Maksymalna liczba wynik贸w (domylnie 5)"
        ] = MAX_SEARCH_RESULTS,
    ) -> str:
        """
        Wyszukuje informacje w Internecie.

        Args:
            query: Zapytanie do wyszukiwarki
            max_results: Maksymalna liczba wynik贸w

        Returns:
            Sformatowana lista wynik贸w wyszukiwania
        """
        logger.info(
            f"WebSearch: szukanie '{query[:100]}...' (max {max_results} wynik贸w)"
        )

        try:
            # LOW-COST ROUTING: W trybie LOCAL/ECO zawsze wymuszamy darmowe
            # 藕r贸da (DuckDuckGo) aby unikn patnych zapyta.
            force_free = getattr(SETTINGS, "LOW_COST_FORCE_DDG", True)
            use_free_search = force_free and self.ai_mode in ("LOCAL", "ECO")

            # U偶yj Tavily jeli dostpny i nie jestemy w trybie LOCAL/ECO
            if self.tavily_client and not use_free_search:
                try:
                    response = self.tavily_client.search(
                        query=query,
                        max_results=max_results,
                        include_answer=True,
                        include_raw_content=False,
                    )

                    # Formatuj wyniki Tavily
                    output = f"Znaleziono wyniki dla zapytania: '{query}'\n"
                    output += "(藕r贸do: Tavily AI Search)\n\n"

                    # Dodaj AI-generated answer jeli dostpny
                    if response.get("answer"):
                        output += f" Podsumowanie AI:\n{response['answer']}\n\n"

                    results = response.get("results", [])
                    if not results:
                        return f"Nie znaleziono wynik贸w dla zapytania: {query}"

                    output += f" 殴r贸da ({len(results)}):\n\n"
                    for i, result in enumerate(results[:max_results], 1):
                        title = result.get("title", "Brak tytuu")
                        url = result.get("url", "Brak URL")
                        content = result.get("content", "Brak opisu")

                        output += f"[{i}] {title}\n"
                        output += f"URL: {url}\n"
                        # U偶yj staej zamiast hardcoded wartoci
                        output += f"Opis: {content[:MAX_CONTENT_PREVIEW_LENGTH]}...\n\n"

                    logger.info(
                        f"WebSearch (Tavily): znaleziono {len(results)} wynik贸w"
                    )
                    return output.strip()

                except Exception as tavily_error:
                    logger.warning(
                        f"Bd Tavily: {tavily_error}. Przeczam na DuckDuckGo."
                    )
                    # Informuj agenta o fallback
                    fallback_note = "锔 Tavily niedostpny, u偶yto DuckDuckGo\n\n"
                    # Fallback do DuckDuckGo poni偶ej

            # Fallback: U偶yj DuckDuckGo
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=max_results))

            if not results:
                return f"Nie znaleziono wynik贸w dla zapytania: {query}"

            # Formatuj wyniki DuckDuckGo
            output = ""
            # Dodaj notatk o fallback jeli bya pr贸ba u偶ycia Tavily
            if self.tavily_client and "fallback_note" in locals():
                output += fallback_note
            output += f"Znaleziono {len(results)} wynik贸w dla zapytania: '{query}'\n"
            output += "(藕r贸do: DuckDuckGo)\n\n"
            for i, result in enumerate(results, 1):
                title = result.get("title", "Brak tytuu")
                url = result.get("href", "Brak URL")
                snippet = result.get("body", "Brak opisu")

                output += f"[{i}] {title}\n"
                output += f"URL: {url}\n"
                output += f"Opis: {snippet}\n\n"

            logger.info(f"WebSearch (DuckDuckGo): znaleziono {len(results)} wynik贸w")
            return output.strip()

        except Exception as e:
            logger.error(f"Bd podczas wyszukiwania: {e}")
            return f"Wystpi bd podczas wyszukiwania: {str(e)}"

    @kernel_function(
        name="scrape_text",
        description="Pobiera i oczyszcza tekst ze strony internetowej. Zwraca czysty tekst bez reklam i mieci HTML.",
    )
    def scrape_text(
        self,
        url: Annotated[str, "URL strony do pobrania"],
    ) -> str:
        """
        Pobiera i oczyszcza tekst ze strony WWW.

        Args:
            url: URL strony do pobrania

        Returns:
            Czysty tekst ze strony lub komunikat o bdzie
        """
        logger.info(f"WebScrape: pobieranie tekstu z {url}")

        try:
            # Najpierw spr贸buj trafilatura (lepsze czyszczenie)
            downloaded = trafilatura.fetch_url(url)
            if downloaded:
                text = trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                )

                if text and text.strip():
                    # Ogranicz dugo
                    if len(text) > MAX_SCRAPED_TEXT_LENGTH:
                        text = (
                            text[:MAX_SCRAPED_TEXT_LENGTH] + "\n\n[...tekst obcity...]"
                        )

                    logger.info(
                        f"WebScrape: pobrano {len(text)} znak贸w z {url} (trafilatura)"
                    )
                    return f"Tre ze strony {url}:\n\n{text}"

            # Fallback do BeautifulSoup jeli trafilatura zawioda
            logger.warning(
                f"Trafilatura nie zwr贸cia wynik贸w dla {url}, pr贸buj BeautifulSoup"
            )

            response = httpx.get(url, timeout=10, follow_redirects=True)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, "html.parser")

            # Usu skrypty, style, itp.
            for script in soup(["script", "style", "nav", "footer", "header"]):
                script.decompose()

            # Pobierz tekst
            text = soup.get_text(separator="\n", strip=True)

            # Usu puste linie
            lines = [line.strip() for line in text.split("\n") if line.strip()]
            text = "\n".join(lines)

            # Ogranicz dugo
            if len(text) > MAX_SCRAPED_TEXT_LENGTH:
                text = text[:MAX_SCRAPED_TEXT_LENGTH] + "\n\n[...tekst obcity...]"

            if not text.strip():
                return f"Strona {url} nie zawiera wystarczajcej iloci tekstu lub jest niedostpna."

            logger.info(
                f"WebScrape: pobrano {len(text)} znak贸w z {url} (BeautifulSoup)"
            )
            return f"Tre ze strony {url}:\n\n{text}"

        except httpx.TimeoutException:
            logger.error(f"Timeout podczas pobierania {url}")
            return f"Przekroczono limit czasu podczas pobierania {url}"
        except httpx.HTTPStatusError as e:
            logger.error(f"Bd HTTP podczas pobierania {url}: {e}")
            return f"Bd HTTP podczas pobierania {url}: {e.response.status_code}"
        except Exception as e:
            logger.error(f"Bd podczas scrapowania {url}: {e}")
            return f"Nie udao si pobra treci z {url}: {str(e)}"

    @kernel_function(
        name="search_and_scrape",
        description="Wyszukuje informacje w Internecie i automatycznie pobiera tre z najlepszych wynik贸w. Zwraca skonsolidowan wiedz z wielu 藕r贸de.",
    )
    def search_and_scrape(
        self,
        query: Annotated[str, "Zapytanie do wyszukiwarki"],
        num_sources: Annotated[int, "Liczba stron do pobrania (domylnie 3)"] = 3,
    ) -> str:
        """
        Wyszukuje i pobiera tre z najlepszych wynik贸w.

        Args:
            query: Zapytanie do wyszukiwarki
            num_sources: Liczba stron do pobrania

        Returns:
            Skonsolidowana wiedza z wielu 藕r贸de
        """
        logger.info(
            f"WebSearchAndScrape: szukanie i pobieranie {num_sources} 藕r贸de dla '{query[:100]}...'"
        )

        try:
            # Ogranicz liczb 藕r贸de
            num_sources = min(num_sources, MAX_SEARCH_RESULTS)

            # Wyszukaj
            with DDGS() as ddgs:
                results = list(ddgs.text(query, max_results=num_sources))

            if not results:
                return f"Nie znaleziono wynik贸w dla zapytania: {query}"

            # Pobierz tre z ka偶dego wyniku
            consolidated = f"Wyniki wyszukiwania dla: '{query}'\n\n"
            total_length = 0

            for i, result in enumerate(results, 1):
                url = result.get("href", "")
                title = result.get("title", "Brak tytuu")

                if not url:
                    continue

                consolidated += f"\n{'=' * 80}\n"
                consolidated += f"殴RDO {i}: {title}\n"
                consolidated += f"URL: {url}\n"
                consolidated += f"{'=' * 80}\n\n"

                # Pobierz tre
                content = self.scrape_text(url)

                # Sprawd藕 limity
                if total_length + len(content) > MAX_TOTAL_CONTEXT_LENGTH:
                    logger.warning(
                        f"Osignito limit cakowitej dugoci tekstu po {i} 藕r贸dach"
                    )
                    consolidated += "\n[...osignito limit dugoci, pozostae 藕r贸da pominite...]\n"
                    break

                consolidated += content + "\n\n"
                total_length += len(content)

            logger.info(
                f"WebSearchAndScrape: pobrano {total_length} znak贸w z {len(results)} 藕r贸de"
            )
            return consolidated.strip()

        except Exception as e:
            logger.error(f"Bd podczas search_and_scrape: {e}")
            return f"Wystpi bd podczas wyszukiwania i pobierania: {str(e)}"


def _get_tavily_client_class() -> Optional[type]:
    """aduje TavilyClient na 偶danie."""

    try:  # pragma: no cover - zale偶ne od rodowiska
        module = import_module("tavily")
        return getattr(module, "TavilyClient")
    except Exception:  # pragma: no cover
        return None
