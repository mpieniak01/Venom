name: venom-release

# End-user profile:
# - pulls prebuilt backend/frontend images from GHCR
# - starts Ollama locally
# - intended for trusted/private networks only
services:
  ollama:
    image: ${OLLAMA_IMAGE:-ollama/ollama:0.16.1}
    container_name: venom-ollama-release
    ports:
      - "11434:11434"
    environment:
      OLLAMA_HOST: ${OLLAMA_HOST:-0.0.0.0}
      OLLAMA_CONTEXT_LENGTH: ${OLLAMA_CONTEXT_LENGTH:-32768}
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-2}
      OLLAMA_MAX_QUEUE: ${OLLAMA_MAX_QUEUE:-256}
      OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION:-1}
      OLLAMA_KV_CACHE_TYPE: ${OLLAMA_KV_CACHE_TYPE:-q8_0}
      OLLAMA_LOAD_TIMEOUT: ${OLLAMA_LOAD_TIMEOUT:-10m}
      OLLAMA_NO_CLOUD: ${OLLAMA_NO_CLOUD:-1}
    volumes:
      - ollama-data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 15s
      timeout: 10s
      retries: 20
      start_period: 20s
    restart: unless-stopped

  backend:
    image: ${BACKEND_IMAGE:-ghcr.io/mpieniak01/venom-backend:v1.5.0}
    container_name: venom-backend-release
    environment:
      VENOM_RUNTIME_PROFILE: ${VENOM_RUNTIME_PROFILE:-light}
      ENV: production
      URL_SCHEME_POLICY: ${URL_SCHEME_POLICY:-auto}
      AI_MODE: LOCAL
      ACTIVE_LLM_SERVER: ${ACTIVE_LLM_SERVER:-ollama}
      LLM_SERVICE_TYPE: local
      LLM_LOCAL_ENDPOINT: ${LLM_HTTP_SCHEME:-http}://ollama:11434/v1
      LLM_MODEL_NAME: ${OLLAMA_MODEL:-gemma3:4b}
      LLM_WARMUP_ON_STARTUP: ${LLM_WARMUP_ON_STARTUP:-true}
      VENOM_OLLAMA_PROFILE: ${VENOM_OLLAMA_PROFILE:-balanced-12-24gb}
      # Backend uses 0/empty as sentinel => resolve from VENOM_OLLAMA_PROFILE.
      # Container-level OLLAMA_* above tune the server defaults.
      OLLAMA_CONTEXT_LENGTH: ${OLLAMA_CONTEXT_LENGTH:-0}
      OLLAMA_NUM_PARALLEL: ${OLLAMA_NUM_PARALLEL:-0}
      OLLAMA_MAX_QUEUE: ${OLLAMA_MAX_QUEUE:-0}
      OLLAMA_FLASH_ATTENTION: ${OLLAMA_FLASH_ATTENTION:-1}
      OLLAMA_KV_CACHE_TYPE: ${OLLAMA_KV_CACHE_TYPE:-}
      OLLAMA_RETRY_MAX_ATTEMPTS: ${OLLAMA_RETRY_MAX_ATTEMPTS:-2}
      OLLAMA_RETRY_BACKOFF_SECONDS: ${OLLAMA_RETRY_BACKOFF_SECONDS:-0.35}
      SUMMARY_STRATEGY: heuristic_only
    depends_on:
      ollama:
        condition: service_started
        required: false
    ports:
      - "8000:8000"
    volumes:
      - venom-data:/app/data
      - venom-workspace:/app/workspace
      - venom-logs:/app/logs
    healthcheck:
      test: ["CMD", "curl", "-fsS", "http://localhost:8000/healthz"]
      interval: 15s
      timeout: 5s
      retries: 10
    restart: unless-stopped

  frontend:
    image: ${FRONTEND_IMAGE:-ghcr.io/mpieniak01/venom-frontend:v1.5.0}
    container_name: venom-frontend-release
    environment:
      NEXT_PUBLIC_ENV: ${ENV:-production}
      URL_SCHEME_POLICY: ${URL_SCHEME_POLICY:-auto}
      NEXT_PUBLIC_URL_SCHEME_POLICY: ${URL_SCHEME_POLICY:-auto}
      API_PROXY_TARGET: ${HTTP_SCHEME:-http}://backend:8000
      NEXT_PUBLIC_API_BASE: ${HTTP_SCHEME:-http}://localhost:8000
      NEXT_PUBLIC_WS_BASE: ${WS_SCHEME:-ws}://localhost:8000/ws/events
    depends_on:
      backend:
        condition: service_healthy
    ports:
      - "3000:3000"
    healthcheck:
      test: ["CMD", "node", "-e", "fetch('http://localhost:3000').then(r=>process.exit(r.ok?0:1)).catch(()=>process.exit(1))"]
      interval: 15s
      timeout: 5s
      retries: 10
    restart: unless-stopped

volumes:
  ollama-data:
  venom-data:
  venom-workspace:
  venom-logs:
