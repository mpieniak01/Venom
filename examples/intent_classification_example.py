#!/usr/bin/env python
"""Przyk≈Çad u≈ºycia systemu klasyfikacji intencji."""

import asyncio
import tempfile
from pathlib import Path

from venom_core.core.intent_manager import IntentManager
from venom_core.core.models import TaskRequest
from venom_core.core.orchestrator import Orchestrator
from venom_core.core.state_manager import StateManager


async def example_direct_classification():
    """Przyk≈Çad bezpo≈õredniej klasyfikacji intencji."""
    print("=" * 60)
    print("PRZYK≈ÅAD 1: Bezpo≈õrednia klasyfikacja IntentManager")
    print("=" * 60)

    # Uwaga: Ten przyk≈Çad wymaga dzia≈ÇajƒÖcego lokalnego LLM lub klucza OpenAI
    # W przeciwnym razie u≈ºyj test√≥w z mockami

    manager = IntentManager()

    test_inputs = [
        "Napisz funkcjƒô w Pythonie do sortowania listy",
        "Co to jest GraphRAG i jak dzia≈Ça?",
        "Witaj Venom, jak siƒô masz?",
    ]

    for user_input in test_inputs:
        try:
            intent = await manager.classify_intent(user_input)
            print(f"\nWej≈õcie: {user_input}")
            print(f"Intencja: {intent}")
        except Exception as e:
            print(f"\nB≈ÇƒÖd dla '{user_input}': {e}")
            print("(Upewnij siƒô, ≈ºe lokalny LLM jest uruchomiony)")


async def example_orchestrator_usage():
    """Przyk≈Çad u≈ºycia z Orchestrator."""
    print("\n" + "=" * 60)
    print("PRZYK≈ÅAD 2: Klasyfikacja przez Orchestrator")
    print("=" * 60)

    # Utw√≥rz tymczasowy plik stanu
    tmp_file = tempfile.NamedTemporaryFile(
        prefix="venom_example_state_",
        suffix=".json",
        delete=False,
    )
    state_file = Path(tmp_file.name)
    tmp_file.close()
    state_manager = StateManager(state_file_path=str(state_file))
    orchestrator = Orchestrator(state_manager)

    # Wy≈õlij zadanie
    request = TaskRequest(content="Zrefaktoruj ten kod Python")
    response = await orchestrator.submit_task(request)

    print(f"\nZadanie utworzone: {response.task_id}")
    print("Status:", response.status)

    # Poczekaj na zako≈Ñczenie
    print("\nOczekiwanie na klasyfikacjƒô...")
    await asyncio.sleep(3)

    # Pobierz wynik
    task = state_manager.get_task(response.task_id)
    if task:
        print(f"\nStatus ko≈Ñcowy: {task.status}")
        print(f"Wynik: {task.result}")
        print("\nLogi:")
        for log in task.logs:
            print(f"  - {log}")

    # Cleanup
    await state_manager.shutdown()
    state_file.unlink(missing_ok=True)


async def main():
    """G≈Ç√≥wna funkcja przyk≈Çadu."""
    print("\nüêç VENOM - Przyk≈Çad Klasyfikacji Intencji üß†\n")
    print("Ten przyk≈Çad pokazuje jak dzia≈Ça system rozpoznawania intencji.")
    print("Wymaga dzia≈ÇajƒÖcego lokalnego LLM (np. Ollama) lub klucza OpenAI.\n")

    try:
        # Przyk≈Çad 1: Bezpo≈õrednia klasyfikacja
        await example_direct_classification()

        # Przyk≈Çad 2: U≈ºycie z Orchestrator
        await example_orchestrator_usage()

    except Exception as e:
        print(f"\n‚ùå B≈ÇƒÖd: {e}")
        print("\nUpewnij siƒô, ≈ºe:")
        print("1. Lokalny serwer LLM jest uruchomiony (np. 'ollama serve')")
        print("2. Model jest pobrany (np. 'ollama pull phi3')")
        print("3. Plik .env jest poprawnie skonfigurowany")


if __name__ == "__main__":
    asyncio.run(main())
