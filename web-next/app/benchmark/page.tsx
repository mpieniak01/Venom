"use client";

import { useState } from "react";
import { Gauge } from "lucide-react";
import { Panel } from "@/components/ui/panel";
import { SectionHeading } from "@/components/ui/section-heading";
import { BenchmarkConfigurator } from "@/components/benchmark/benchmark-configurator";
import { BenchmarkConsole } from "@/components/benchmark/benchmark-console";
import { BenchmarkResults } from "@/components/benchmark/benchmark-results";
import { useModels } from "@/hooks/use-api";
import type {
  BenchmarkConfig,
  BenchmarkLog,
  BenchmarkModelResult,
  BenchmarkStatus,
} from "@/lib/types";

// Sta≈Çe dla symulacji benchmarku
const SIMULATION_MODEL_LOAD_DELAY_MS = 1500;
const SIMULATION_QUESTION_DELAY_MS = 800;
const OOM_PROBABILITY = 0.15; // 15% szans na OOM
const ERROR_PROBABILITY = 0.1; // 10% szans na b≈ÇƒÖd
const MIN_RESPONSE_TIME_MS = 800;
const RESPONSE_TIME_RANGE_MS = 2000;
const MIN_TOKENS_PER_SEC = 10;
const TOKENS_PER_SEC_RANGE = 40;
const MIN_VRAM_MB = 2048;
const VRAM_RANGE_MB = 4096;

export default function BenchmarkPage() {
  const { data: modelsData, loading: modelsLoading } = useModels(15000);
  const [status, setStatus] = useState<BenchmarkStatus>("idle");
  const [logs, setLogs] = useState<BenchmarkLog[]>([]);
  const [results, setResults] = useState<BenchmarkModelResult[]>([]);

  const addLog = (message: string, level: BenchmarkLog["level"] = "info") => {
    setLogs((prev) => {
      const newLog: BenchmarkLog = {
        timestamp: new Date().toISOString(),
        message,
        level,
      };
      // U≈ºywamy concat zamiast spread dla lepszej wydajno≈õci
      return prev.concat(newLog);
    });
  };

  /**
   * Symulacja benchmarku - funkcja demonstracyjna
   *
   * Ta funkcja generuje losowe wyniki dla cel√≥w demonstracyjnych.
   * U≈ºywa sta≈Çych zdefiniowanych na g√≥rze pliku (SIMULATION_MODEL_LOAD_DELAY_MS,
   * SIMULATION_QUESTION_DELAY_MS, OOM_PROBABILITY, ERROR_PROBABILITY, itp.)
   * do kontrolowania symulacji.
   *
   * W finalnej implementacji bƒôdzie zastƒÖpiona przez prawdziwe wywo≈Çania API:
   * - POST /api/v1/models/benchmark/start
   * - WebSocket/SSE dla live log√≥w
   * - GET /api/v1/models/benchmark/{id}
   */
  const runBenchmark = async (config: BenchmarkConfig) => {
    setStatus("running");
    setLogs([]);
    setResults([]);

    addLog(`Rozpoczynam benchmark dla runtime: ${config.runtime}`);
    addLog(`Wybrane modele: ${config.models.join(", ")}`);
    addLog(`Liczba pyta≈Ñ testowych: ${config.num_questions}`);

    try {
      const mockResults: BenchmarkModelResult[] = [];

      for (let i = 0; i < config.models.length; i++) {
        const modelName = config.models[i];
        addLog(`[${i + 1}/${config.models.length}] ≈Åadowanie modelu: ${modelName}...`);

        // Symulacja op√≥≈∫nienia ≈Çadowania (zastƒÖpi prawdziwy czas ≈Çadowania modelu)
        await new Promise((resolve) => setTimeout(resolve, SIMULATION_MODEL_LOAD_DELAY_MS));

        addLog(`Model ${modelName} za≈Çadowany. Rozpoczynam generowanie odpowiedzi...`);

        // Symulacja testowania (zastƒÖpi prawdziwe wywo≈Çania API do modelu)
        for (let q = 1; q <= config.num_questions; q++) {
          addLog(
            `  Generowanie odpowiedzi ${q}/${config.num_questions} dla ${modelName}...`
          );
          await new Promise((resolve) => setTimeout(resolve, SIMULATION_QUESTION_DELAY_MS));
        }

        // Symulacja wynik√≥w - losowe warto≈õci dla demonstracji
        const isOOM = Math.random() < OOM_PROBABILITY;
        const isError = !isOOM && Math.random() < ERROR_PROBABILITY;

        const result: BenchmarkModelResult = {
          model_name: modelName,
          avg_response_time_ms: isOOM || isError ? 0 : MIN_RESPONSE_TIME_MS + Math.random() * RESPONSE_TIME_RANGE_MS,
          tokens_per_sec: isOOM || isError ? 0 : MIN_TOKENS_PER_SEC + Math.random() * TOKENS_PER_SEC_RANGE,
          max_vram_mb: MIN_VRAM_MB + Math.random() * VRAM_RANGE_MB,
          status: isOOM ? "oom" : isError ? "error" : "success",
          error_message: isError ? "Connection timeout" : undefined,
        };

        mockResults.push(result);

        if (isOOM) {
          addLog(
            `‚ùå Model ${modelName} przekroczy≈Ç limit VRAM i spowodowa≈Ç OOM`,
            "error"
          );
        } else if (isError) {
          addLog(`‚ö†Ô∏è Model ${modelName} zako≈Ñczy≈Ç siƒô b≈Çƒôdem`, "warning");
        } else {
          addLog(
            `‚úÖ Model ${modelName} uko≈Ñczony - ${result.avg_response_time_ms.toFixed(0)}ms avg, ${result.tokens_per_sec.toFixed(2)} tok/s`,
            "info"
          );
        }
      }

      setResults(mockResults);
      addLog("üéâ Benchmark zako≈Ñczony pomy≈õlnie!", "info");
      setStatus("completed");
    } catch (error) {
      const errorMessage = error instanceof Error
        ? error.message
        : "Nieznany b≈ÇƒÖd podczas wykonywania benchmarku. Sprawd≈∫ logi systemu lub spr√≥buj ponownie.";
      addLog(`B≈ÇƒÖd podczas benchmarku: ${errorMessage}`, "error");
      setStatus("failed");
    }
  };

  const handleStart = async (config: BenchmarkConfig) => {
    await runBenchmark(config);
  };

  // Przygotuj listƒô modeli do wyboru
  const availableModels =
    modelsData?.models.map((model) => ({
      name: model.name || "unknown",
      provider: model.provider || "vllm",
    })) || [];

  return (
    <div className="space-y-6">
      {/* Header */}
      <SectionHeading
        as="h1"
        size="lg"
        eyebrow="Benchmark Control"
        title="Panel Benchmarkingu"
        description="Testuj wydajno≈õƒá modeli i por√≥wnaj ich parametry (czas odpowiedzi, tokens/sec, u≈ºycie VRAM)"
        rightSlot={<Gauge className="page-heading-icon" />}
      />

      <div className="grid gap-6 lg:grid-cols-2">
        {/* Konfigurator */}
        <Panel
          eyebrow="Krok 1"
          title="Konfiguracja testu"
          description="Wybierz runtime, modele i liczbƒô pyta≈Ñ"
        >
          {modelsLoading ? (
            <div className="flex items-center justify-center py-8">
              <div className="h-6 w-6 animate-spin rounded-full border-2 border-violet-500 border-t-transparent" />
              <span className="ml-3 text-sm text-zinc-400">
                ≈Åadowanie modeli...
              </span>
            </div>
          ) : (
            <BenchmarkConfigurator
              availableModels={availableModels}
              onStart={handleStart}
              disabled={status === "running"}
            />
          )}
        </Panel>

        {/* Console / Logi */}
        <Panel
          eyebrow="Krok 2"
          title="Postƒôp wykonania"
          description="PodglƒÖd na ≈ºywo log√≥w z test√≥w"
        >
          <BenchmarkConsole logs={logs} isRunning={status === "running"} />
        </Panel>
      </div>

      {/* Wyniki */}
      <Panel
        eyebrow="Krok 3"
        title="Wyniki por√≥wnawcze"
        description="Tabela z metrykami wydajno≈õci dla testowanych modeli"
      >
        <BenchmarkResults results={results} />
      </Panel>
    </div>
  );
}
